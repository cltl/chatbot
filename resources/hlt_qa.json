{
    "intents": [
        {
            "num": 15,
            "questions": [
                "WordNet",
                "synonymy",
                "What is WordNet",
                "What is WordNet?",
                "What is the wordnet"
            ],
            "responses": [
                "WordNet is an online lexical reference system based on synonym sets.",
                "WordNet is a web-based lexical system organized by semantical relations. It resembles a dictionary",
                "Wordnet is a system that contains nouns, verbs, adjectives and adverbs. It works with synsets, which are sets of synonyms.",
                "It is a large of English words database contains (Nouns, verbs, adverbs and adjectives) have been built in semantic relation.",
                "WordNet is a large lexical database of English based on psycholinguistic principles, organized conceptually and hierarchically into synonym sets",
                "Wordnet is a large lexical database in which nouns, verbs, adverbs and adjectives are organized by a variety of semantic relations into synonym sets (synsets), which represent one concept.",
                "An online dictionary based on psycholinguistic principles. It contains nouns, verbs,adjectives and adverbs which are separated by their syntactic categories. The words are organized by a variety of semantic relations.",
                "WordNet is an online massive English lexical database developed by Princeton University, in which nouns, verbs, adjectives and adverbs are grouped by some semantic relations-e.g. synonymy, autonomy, hyponymy and meronymy- into synonym sets.",
                "A lexical database with different language versions, based on psycholinguistic principles, which divides the lexicon in nouns, verbs, adjectives, adverbs and function words. One of the ways that it can be used is to represent word senses, which is one of the meanings of a word.",
                "WordNet is an online lexical database of the English language. It is organized into four large groups: nouns, verbs, adjectives and adverbs, which are in turn organized in so-called synsets. WordNet is used as a tool for natural language processing and computational linguistics.",
                "WordNet is a lexical database that connects semantic relations between words in a language. In a wordnet, words are POS tagged and it is found the synonyms, hyponyms, and meronyms of them. Words are grouped semantically and it includes short definitions and usage examples. It is a practical instrument to apply NLP. ",
                "WordNet was created after a research project at Princeton University. It was created in the 80’s of the previous century, and has grown ever since. It was a new word search engine after dictionaries and thesauri. Words in WordNet are organized based on semantic relations and lexical relations and the available categories are nouns, verbs, adverbs and adjectives.",
                "WordNet is a semantic database of nouns, verbs and adjectives that are organised based on semantic relations. WordNet is designed in order to provide an effective combination of traditional lexicography and modern high-speed communication. WordNet is a very smart program that tries to overcome ambiguity by making use of polysemy, and that describes word variation using synonyms and hyponyms. ",
                "WordNet is a type of online lexicon, organized based on relations between word meanings (and, to a smaller extent, relations between word forms), not on an alphabetical or similar traditional principle. WordNet divides up its lexical entries into nouns, verbs, adjectives and adverbs, reflecting psycholinguistic evidence that our mental lexicons seem to handle these word classes separately in real life as well.",
                "​WordNet is an online lexical database created based on the content and organization of the mental lexicon as proposed by psycholinguistic models and principles. It attempts to combine the traditional dictionary and the current computational efficiency to offer a testable tool for psycholinguistic hypothesis. Words are divided into nouns, verbs, adverbs and adjectives, organized according to relations such as synonymy, antonymy, meronymy, etc."
            ],
            "category": "wordnet"
        },
        {
            "num": 14,
            "questions": [
                "vector",
                "embedding",
                "word vectors",
                "word embeddings",
                "vector semantics",
                "What are vectors?",
                "What is vector semantics",
                "What are vector semantics",
                "What is vector semantics?",
                "What are vector semantics?",
                "vector semantics, vector model, word embbeding"
            ],
            "responses": [
                "Two words are similar if they have similar word context",
                "Vector semantics is a model that represents the meaning of words in a multi-dimensional vector space",
                "As stated before, a word as a vector is a list of numbers and vector semantics shows a word in a semantic space. ",
                "Vector semantics is learning representations (vectors) of the meaning of words directly from their distributions in texts.",
                "A word is represented as a point in multidemnsional space. Similar words are nearby eachother in space. The vectors used to represent meaning are called embeddings. ",
                "Vector semantics are a model for determining word meaning. A vector is a point (similar to a coordinate) in a N-dimensional space, determined by the occurrences of neighboring words.",
                " Vector semantics is the idea to represent a word as a certain point in a (imgaginary) dimensional semtnaic space. In vector semantics, a word is represented as a sequence of numbers. ",
                "Vector semantics considers a word in multi-dimensional vector space where the shorter is the distance between the word vectors the closer is the meaning. Those verctors representing the words are called embeddings, because each word is embedded in a particular vector space",
                "Vector semantics is a computational model that define a word by its distribution, i.e. its neighbouring words and grammatical environment. Words are represented as vectors, that is, a list of numbers that encode the statistical association strength between a word and a context.",
                "vectors make it possible to structure contextual representation into numerical data. By taking into account the distributional hypothesis, vectors distances are used to measure the similarity between words. Also social context words should be defined by how those words are used by people in a social context. ",
                "Vector semantics is a model that represents words exclusively through their context (i.e. the words that surround them). Each word is a vector with as many dimentions as there are words in the vocabulary, and each value displays the frequency with which the target word co-occurs with each word in the vocabulary.",
                "This is a view where one is able to define a word with vectors. To be more precise, words are used to be shown as a point in a semantic space that is multi-dimensional.  A vector is a data structure that is numerical and able to contextualise representations. Vectors that are used to represent words are actually called ‘embeddings’. One of the big advantages of Vector Semantics is that it is able to integrate word similarity in the model.",
                "Vector semantics is the representation of meaning of a word in a vector, which contains whether or not the word appeared in the context (of a certain range) of other words. The comparison of these vectors can say something about how similar these words are. Note that this method handles words based on association(whether or not they appear in the same context), rather than meaning.A vector representation for a word is often called a word embedding.",
                "Vector semantics builds on the distributional hypothesis: in some way the context of a word is informative when trying to capture word meaning. Word vectors are usually assembled by enumerating all words that occur in the context window of a target word and collecting the frequency counts as values in the vector. This allows us to easily determine similarity between word vectors, to automatically draw analogy through vector relations and carry out a range of different operations with the vectors."
            ],
            "category": "vector semantics"
        },
        {
            "num": 13,
            "questions": [
                "Turing test",
                "Alan Turing",
                "Imitation Game",
                "What is Turing test",
                "What is a turing test?",
                "What is the Turing test",
                "What is the Turing Test?"
            ],
            "responses": [
                "A verbal test designed to test whether a machine can pass as a human.",
                "Binary NB is a variant of sentiment analysis in which the frequency of a word seems less important than whether the word appears or not. A difference in this variant is that duplicate words are removed before they are linked together into a document.",
                "A Turing Test is a method of inquiry in artificial intelligence (AI) for determining whether or not a computer is capable of thinking like a human being. The test is named after Alan Turing, the founder of the Turing Test and an English computer scientist.",
                "The Turing Test is a test to determine if a computer can think. In a game-type setting an interrogator needs to figure out which one of the hidden participants is human and which one is a computer. If the distinction can’t be made, the computer is seen as intelligent.",
                "The Turing Test is a test conducted by Alan Turing, in which he wanted to show that if a machine is indistinguishable from a human, in a setting where the interrogator’s assessment is based on written conversations, then the conclusion can be drawn that a machine can think.",
                "A test introduced by Alan Turing in 1950, in which a human and a machine are interrogated by another human, who is trying to figure out which one is the machine is and who is the man. The purpose of this interrogation is to find out if the machine has reached the level of human intelligence as far as one can tell.",
                "To test whether a machine is intelligent, Alan Turing has suggested the following test. Have a judge, a human, who is asking questions from an anonymous interviewee solely in writing. This anonymous interviewee is a machine. If the machine can convince the judge with its answers that it is human, it has passed the Turing test and it can be said that it has human intellect.",
                "A test that questions if machines can think. Turing developed the 'imitation game', by which the language use of a computer would be the basis for answering the question whether a computer can think or not. The task of a machine is to fool an interrogater, by responding as a human person would respond. If a computer can react in such a way that a person can't distinguish the answers from the computer from a real person, than the computer can think.",
                "The \textit{Turing test} is a test devised to test whether a machine can mimic intelligent human behavior convincingly enough so as to be indistinguishable from an actual human. In this test, a human interrogator is tasked with determining which one of two players is a machine and which one is human. Both players, one human and one machine, have the task of convincing the interrogator that they are human. A machine passes the test if a human is unable to correctly identify the machine.",
                "The Imitation Game is a method proposed by Alan Turing to empirically test whether machines can think. Given the vagueness of the question ‘can machines think?’, Turing suggests that a human interrogator asks questions to both a machine and another human, without seeing or hearing them, and solely based on their answers, decides who is the machine and who is human. If the computer can perform as well as a human in this task, deceiving the interrogator, Turing’s conclusion is that the machine is intelligent.",
                "The Turing Test is a game that tests the intelligence of a machine by establishing whether it can fool humans to believe the machine is human too on the basis of language use. During the game, played by two humans (A and B) and the machine, human A interrogates B and the machine in order to find out which of them is the human and which the machine. If the machine manages to convince A that it is the human, it passes the Turing test, and can be judged as intelligent according to Turing. In other words, Turing claimed that humanlike language use alone can be regarded as a test for intelligence.",
                "A test whether machines could think. The turing test is done by the imitations game, which consists of a game humans play with a machine. When the machine wins the game, it states that the machine could think. A criteria was that language used as humans do shows whether the machine is intelligent. Turing stated that back in the time programming machines was a problem. It is an article which has very progressive thoughts about technology and how it should be in the future. For example ideas of having a learning machine (Turing, 1950). Nowadays there are machines which we can interact with as social entities. People act as if a machine is a real person because of the features that shows that a machine can think and has feelings (Nass & Reeves, 1996). Therefor there has come a long-term focus in the field of designing conversational agents.",
                "The Turing Test is a test designed by A. M. Turing in 1950 to test if a machine could pass as a human and therefore can be assigned the same mental capacities as a human. The test is set up with two humans and a computer. One of the humans has to ask questions through a computer screen and the human and the computer both have to respond to the questions. Afterwards, the interrogator has to decide which of the two is the computer and which is the human. If the computer passes as a human, it can be said that the computer has the same mental capacity on that subject as a human. The Turing Test is important for the research field of AI and formal language technology because it was one of the first test designed to test computers in the same way as humans. It is a strong claim to say that when a computer passes the test, it has the same capacity as the human brain. "
            ],
            "category": "turing test"
        },
        {
            "num": 13,
            "questions": [
                "DH",
                "distributional semantics",
                "Distributional Hypothesis",
                "What is distributional hypothesis",
                "What is Distributional Hypothesis?",
                "What is the distributional hypothesis",
                "What is the distributional hypothesis?",
                "What does the distributional hypothesis state?",
                "distributional hypothesis, distributional meaning"
            ],
            "responses": [
                "Distributional hypothesis is the hypothesis that words in similar contexts are likely to have similar meanings",
                "To find how semantically similar two words are, the linguistic contexts of those words have to be investigated.",
                "The Distributional Hypothesis states that words that occur in similar linguistics contexts are likely to have similar meanings.",
                "Distributional Hypothesis states that similarities of the meaning of two words has to do with the degree of similarities of their contexts.",
                "it's a method to check the similarity between two context and if it was similar then the words which are common should hold the same meaning",
                "The distributional hypothesis is the theory about how words with similar meanings are distributed. How more similar they are, how closesly distributed.",
                "The degree of semantic similarity between two linguistic expressions A and B is a function of the similarity of the linguistic contexts in which A and B can appear.",
                "The distributional hypothesis says that the definition of a word can be defined by the semantics of the surrounding words. It states that words that occur in similar contexts probably have similar meanings.",
                "Distributional Hypothesis is the idea that suggests that if two words are similar in meaning, it is likely that they occur in the same contexts. To quote Firth 'You shall know a word by the company it keeps'.",
                "The hypothesis that words that are similair in meaning also often occur in the same contexts. In other words the degree of semantic similarity is a function of the similarity of the linguistic contexts in which the two words can appear.",
                "This hypothesis states that lexical expressions depend on distributional properties. Meaning that the more similar lexical expressions are the more they will be used in similar semantic contexts. So it is important to look at the environment a word is used in. ",
                "The distributional hypothesis refers to the idea that words that are synonyms (or near-synonyms) often occur in the same context, meaning that the same words surround them in a sentence. The difference between words is reflected in the difference in their environments.",
                "There is an observation that words appearing in the same linguistic contexts share a greater degree of semantic similarity with each other than with other words. This observation then prompts the hypothesis that the linguistic distribution of words constitutes of at least in part their meaning representations. It is debated, however, to what extent linguistic distributions are the actual semantic representations, or if they are only governed by covert semantic content."
            ],
            "category": "distributional hypothesis"
        },
        {
            "num": 11,
            "questions": [
                "synonymy",
                "word relation",
                "What is synonymy",
                "What is synonymy?",
                "What is a synonym?",
                "Which words are synonyms?"
            ],
            "responses": [
                "Synonymy is the resemblance of meaning between words.",
                "Synonymy is similarity of meaning between lexical concepts",
                "Two word forms are generally said to be synonymous if they mean the same thing.",
                "Synonymy is a relation between words, where two or more words contain (roughly) the same semantic value.",
                "Two expressions are synonymous in a linguistic context C if the substitution of one for the other in C does not alter the truth value.",
                "Two expression are synonymous if the substitution of one for the other never changes the value of a sentence in which the substitution is made. Relative to a context.",
                "Synonymy is a lexical relation between word forms which tell us about the similarity in meaning between them. If one word form can be exchanged with another in the same contex without change in meaning of the contex, that means that those two words are synonyms",
                "Synonymy is when two concepts that are synonymous have very similar meanings and can be interchangeable. If a word in a sentence was to be replaced by a synonymous word the value of the sentence would remain true. On WordNet synonymy takes a central role as words definitions are represented by synonyms",
                "This is the similarity of meaning and also one of the most important relations in WordNet. If the substitution of one or the other does not change the actual meaning, the words can be seen as synonymous. This is shown by curly brackets in WordNet. When a set of words are near-synonyms and thus share one common meaning, they are called a synset.",
                "According to official definition, words that substituted with each other in a sentence, do not alter the meaning of said sentence. Absolute synonyms like this are very rare. Especially in the terms of WordNet synonyms are of the same word class, as supported by psycholinguistics evidence. This is a relation between word form rather than word meaning, as the latter is shared by the different forms.",
                "In the strictest definition, words are synonyms if they can replace each other in every context without changing the truth conditions of the sentence. This definition is quite narrow as rarely any words qualify. A weaker definition is more context-dependent, stating that words are synonyms if they can replace each other without changing the truth conditions of the sentence in that particular context."
            ],
            "category": "synonymy"
        },
        {
            "num": 9,
            "questions": [
                "naive bayes classifier",
                "What is naive Bayes classifier",
                "Wat is a Naive Bayes classifier?",
                "What is a naive Bayes classifier",
                "What is a naive bayes classifier?",
                "What is the naive Bayes classifier",
                "What is the Naive Bayes classifier?"
            ],
            "responses": [
                "The Naive Bayes classifier is a generative classifiers, builds a model that predicts the joint probability of a pair from input and label.  ",
                "Naive Bayes is a classification algorithm which is characterized by two assumption in its core - the bag-of-words and the naive Bayes assumption",
                "The Naive Bayes Classifier is a probabilistic model that's used to text classification, based on the assumption that the position of the word does not matter and that the words are conditionally independent of each other.",
                "A Naive Bayes classifier is a simple probabilistic classifier that naively assumes that features are independent of each other, and chooses the right label by combining scores of the prior probability of a label and the document likelihood.",
                "Naive Bayes algorithm is a generative classifier. It builds a model of how a class could generate some input data. Given an observation, they return the class most likely to have generated the observation. Naive Bayes is a probabilistic classifier. Therefore, it makes a simplifying assumption about how the features interact. ",
                "The Naive Bayes classifier is a classification algorithm that, given an observation, analyse its features and thereby predicts which class is most likely to have generated that observation. This algorithm relies on the assumptions of strong independence between features and the irrelevance of the order in which features are positioned.",
                "This a form of classification whereby every feauture gets a say in determining the label wich should be assigned to a label. This is dne by calculating the prior probability of each label, determined by checkin the frequency of each label in the training set. This is based on Bayes's law calculates the probability of an occurance based on the prior occurances. The problem with tis law is that the feautures should not be dependent on eachoher. This is very rare.",
                "A naive Bayes classifier will classify an item based on a vector of its features. The output of the classifier wil be the probability the item belongs to each of the classes. The higher the probability for a class, the more likely the item belongs in this class. These probabilities are computed using Bayes theorem for conditional probabilities, which explains how the probability of an item given a class can be computed. The reason the naive Bayes classifer is called naive, is because it assumes their is no interaction between the features. It assumes all features, indepenent of other features, are relevant for the classification",
                "Naive Bayes is the name of a classifier used in various machine learning applications such as sentiment analysis, part-of-speech tagging and emotion detection. As naive Bayes is a probabilistic classifier, what it aims to do is to return the most likely class label for a given target item (a document, a word, etc.). To assemble a naive Bayes classifier, we first need to determine a set of features it will use to calculate the probabilities of class labels given the target item. Then we pass a training set to the classifier, which in turn will learn what are the probabilities of the individual features appearing if the target item belongs to one or the other classes. The 'naivety' of the classifier comes from the simplifying assumption that the features are completely independent from each other. This is an important assumption to make, otherwise taking the product of the probabilities of the features 'being generated' with each class would be an impossible task."
            ],
            "category": "naive bayes classifier"
        },
        {
            "num": 9,
            "questions": [
                "sentiment",
                "sentiment analysis",
                "What is Sentiment Analysis",
                "What is sentiment analysis?",
                "sentiment analysis, sentiment extraction, sentiment classification, text categorization task"
            ],
            "responses": [
                "Sentiment analysis is a classification task, labeling a text as positive or negative towards a certain object.",
                "Sentiment analysis classifies a text as reflecting the positive or negative orientation (sentiment) that a writer expresses toward some object.",
                "Sentiment analysis is the classification task of indication if the writer ecpressing positive, neutral or negative sentiment regarding a topic of a text.",
                "A classification task, that can be solved by a Naive Bayes Classifier. The task is to classify the sentiment of a text, as either a positive or negative one.",
                "Sentiment analysis is a categorization task that focuses on extracting sentiment from a text by labelling the positive or negative orientation of the author.",
                "Sentiment analysis is a text categorization task, which extracts the sentiment of a given text, the positive or negative orientation from a writer towards a certain object. ",
                "Sentiment analysis is the task of extracting the positive and negative atitudes and opinions expressed through text towards something, such as a movie, a product, or a politician.",
                "A text categorization task where the sentiment of the author towards an object is extracted. The simplest form is a binnary classification where the sentiment is classified positive or negative",
                "The task of sentiment analyis is to extract from a document the attitude of the author of the document towards its subject material. This is a crucial tool for a large number of fields, as knowing what people (both on the internet and elsewhere) think of one's product, ideas or ideology can be crucial to make a good analysis of a sales or marketing strategy, a policy change or a political campaign. The task, just like most NLP applications is far from trivial. There are numerous ways in which words which normally have positive or negative connotations might in fact signify the reverse of the normal affective content. Irony and sarcasm are often used rhetorical devices, but even the presence of a simple 'not' can perplex the sentiment classifier."
            ],
            "category": "sentiment analysis"
        },
        {
            "num": 8,
            "questions": [
                "strong AI",
                "What is strong AI",
                "What is strong AI?"
            ],
            "responses": [
                "Strong AI is a view that holds that computers are able to think on a human level, and above human level",
                "Strong AI is a movement within AI that believe in developing machines that have real human thoughts and feelings.",
                "Strong AI is a theoretical current in AI. It states that machines can be intelligent by equating thought to the manipulation of symbols.",
                "People who support this point of view claim that thinking is nothing more than handling formal signs while not relating them to a semantic meaning. Therefore it is assumed that programming a machine that can handle signs creates actual thinking.",
                "Strong AI means that thinking is done by manipulating formal symbols. Similar to what a computer does with input. The Strong AI view is that the Mind is equal to the program, and the brain  is equal to the hardware; to show the similarities between a person and a machine",
                "An approach that supports the idea that thinking is the manipulation of formal symbols and that is exactly what a computer does as well. According to this view, there is nothing physical or biological about the human mind and it is therefore independent of the human brain.",
                "Strong AI is a program that can perfectly duplicate human cognition. Proponents for the existence of strong AI maintain that thinking is merely the manipulation of symbols. As computer programs work by manipulating arbitrary symbols (on a low level 0’s and 1’s), it would be possible to build a program that can carry out human thinking.",
                "Strong Artificial Intelligence is a term that arose around the same time as the Turing Test. It claims that human thinking can be seen as the same as machine processing, because it both manipulates formal symbols. Strong AI is in line with the Turing Test, because the Test is merely based on the manipulation of symbols. When a machine succeeds to do this in such a way that is resembles human responses, it passes the test."
            ],
            "category": "strong ai"
        },
        {
            "num": 8,
            "questions": [
                "model",
                "vector",
                "What is word2vec",
                "What is word2vec?"
            ],
            "responses": [
                "Word2vec is a group of models used for representing word meaning by word embeddings",
                "word2vec is a tool of word embedding which produces word vectors after given a text corpus.",
                "The word2vec family of models is an efficient algorithm used to compute dense embeddings. Words are represented through the use of vectors that are short and dense which work better in NLP processes.",
                "A software package with two models: skip-gram and CBOW. It uses short, dense vectors, so with mostly non-zeroes. Skip-gram tries to predict words according to the context, and CBOW uses a similar method to predict context based on a target word.",
                "Word2vec is a family of models, including skip-gram and CBOW, to compute dense embeddings. Embeddings are vectors for representing words (embedded in a particular vector space), in the case of word2vec consisting of the learned classifier regression weights on a binary prediction task.",
                "word2vec is a package that contains algorithms that calculate word embeddings. First it calculates the probability for a target word to appear close to a context word and then it learns the embedding representation as it compares it to embeddings of words that appear in the same context and such of noise words. ",
                "Word2vec a model in which dense vectors are constructed. These are short vectors, but have a lot of properties that can be useful. These models are easily available online and are pretty fast. There is no standard counting involved, but instead a classifier is trained to analyse if a word is likely to appear near another word. It is similar to neural networks, but it is a simpler method as word2vec makes both the task and the architecture simpler.",
                "Word2vec is another model for representing co-occurrence of words. Compared to models like tf-idf, word2vec is much simpler, with shorter vectors and denser (more values are non-zero). Instead of counting how often each word will appear near a word x, word2vec simplifies the task by asking if word w is likely to appear near word x, the learned weights of this task are used as the vector embeddings. This algorithm usually contains far less dimensions and parameters, making it easy for NLP tasks."
            ],
            "category": "word2vec"
        },
        {
            "num": 8,
            "questions": [
                "What is supervised machien learning",
                "what is supervised machine learning",
                "What is supervised machine learning?"
            ],
            "responses": [
                "An algorithm that learns based on a labeled dataset, meaning that the set contains instances with a right answer.",
                "Being given certain input-output pairs which serve as examples, the algorithm's objective it to provide the right output when given some input",
                "Supervised machine learning uses a dataset (a collection of labeled examples) to produce a model that takes a feature vector as input and provides output to derive the label for this feature vector.",
                "Supervised machine learning is to train a computer by providing the input observations among which each is related to the correct output, in order to let the computer correspond a new observation with the correct output.",
                "In the context of classification, a supervised machine learning algorithm provides labels by finding patterns in a small set of 'correct' samples that is annotated by humans with the desired output, and generalizes this to unseen data.",
                "Supervised machine learning is a type of learning where there is data made up of input observations and these input observations are linked with one correct output. The end goal is being able to learn to arrange a correct output from an input observation.",
                "In supervised machine learning there is a data set of input observations, each associated with some correct output, called a ‘supervision signal'. The goal of the algorithm is to learn how to map from a new observation to a correct output. An ideal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a reasonable way.",
                "Supervised machine learning is a method to make a machine perform a task correctly, with interference of something that is not part of the machine (the supervisor). Mostly the data of which a machine has to learn, is separated in a training set and a test set. The training set will include correct answers, provided by the supervisor. The machine will adjust its parameters going over the training data until the best performance on the training data is reached. This performance can be measured because the correct labels are present. Then the machine will be evaluated using an F-measure on an unseen test set, which wich the labels are not present."
            ],
            "category": "supervised machine learning"
        },
        {
            "num": 8,
            "questions": [
                "class",
                "naive bayes",
                "What is classification",
                "What is classification?"
            ],
            "responses": [
                "Classification is learning to model a class based on a given observation.",
                "Classification is the task of labeling a certain input with a class label.",
                "Classification is giving some input data the right label (for example, POS tagging is actually a classification)",
                "In language processing, classification is an operation that involves the observation of a text and the assignment of class labels to it and to its elements.",
                "Classification entails tasks where a group of documents have to be assigned to a category, for example spam/not spam, positive or negative sentiment, or language.  ",
                " Assigning a label to a certain input. This can be done by first creating classifiers. It is important to decide which features a classifier has. Through a function in python you can see a description of the features after this  ",
                "A common task in data mining is classification of an input. This means putting a label on this input, so that we know what class it belongs to. For example, a self driving car should be able to make the distinction between a pedestrian and another car. This is a form of classification",
                "Classification in AI is the task of assigning the correct label to a given input. Once trained, it takes an input and classifies it into one of a set of classes/labels, according to the relevant input features. Examples of classification tasks include detecting spams, defining a text's topic, identifying positive and negative opinions towards an object, and attributing text authorship."
            ],
            "category": "classification"
        },
        {
            "num": 7,
            "questions": [
                "What is hyponymy",
                "What is hyponymy?",
                "Which words are hyponyms?",
                "semantic hierarchical relation"
            ],
            "responses": [
                "Hyponymy is a semantic hierarchical relation between word meanings.",
                "Hyponymy is, semantically speaking, the subordinate of a higher rank or class which is its superordinate. e.g. a table is a hyponymy of furniture.",
                "Hyponymy expresses the relations between words x and y where x is a hyponym of y if x is a type of y, or if all x-es are y-s, but not the other way around.",
                "Hyponymy refers to the relation where one word (or in this case sense) is a subclass of the other. For example, a pine is a kind of tree, where pine is the hyponym and tree is the hypernym.",
                "A hyponym is in a type of relationship with its hypernym. For example, snake, lizard, and turtle are all hyponyms of reptiles (their hypernym); which, in turn, is a hyponym of animal. Other names for hypernym include umbrella term and blanket term.",
                "It concerns relations between word meanings as opposed to forms. If a word meaning can be described as a kind of another word meaning, then the latter is its superordinate. All the word meanings in this hierarchy would share a feature but would be further defined by more specific features.",
                "While the two terms above lexical relations based on word forms, hyponymy has to do with word meanings and the semantic relation between them. It can be seen as a sub-class, so ‘cat’ is the hyponym of ‘animal’, while ‘animal’ is then the hypernym of ‘cat’. In WordNet, this is a principle that is central for nouns. "
            ],
            "category": "hyponymy"
        },
        {
            "num": 7,
            "questions": [
                "What is Recall",
                "What is recall?",
                "What does recall mean? ",
                "What is the Recall metric"
            ],
            "responses": [
                "the number of true positives/the number of true positives plus the number of false negatives",
                "Recall is the number of true positives divided by the sum of the number of true positives and false negatives.",
                "Recall is a measure for assessing classifiers which gives the percentage of items that were labeled by the classifier and that are actually in the input",
                "Using an e-mail spam filter as an analogy, the recall would be the number of correctly labeled spam e-mails as a percentage of the total number of e-mails labeled as spam.",
                "Recall is an evaluation metric that indicates the percentage of the total number of true positives that were identified by the program, and can be calculated as follows: true positives / true positives + false negatives ",
                "A measure of performance of a bayesian classifier. It shows the amount of true positives (correctly identified trues) as a percentage of all that is, in fact, true (true positives + false negatives) . Combined with the accuracy measure, this makes the F-measure.",
                "Recall is a type of accuracy measure. It measures the percentage of the outcomes that is correctly labeled as positive for a measuring system against all the outcomes that are actually positive, but might have been labeled negative. It is often a artificial system versus a human measuring system."
            ],
            "category": "recall"
        },
        {
            "num": 6,
            "questions": [
                "word matrix",
                "lexical matrix",
                "What is Lexical Matrix",
                "What is classification",
                "What is a lexical matrix",
                "What is a Lexical Matrix?"
            ],
            "responses": [
                "A lexical matrix is a way of mapping of words and meanings.",
                "Lexical matrix is a mapping between word forms (physical manifestations of words) and word meanings (concepts expressed by words)",
                " It creates a combination of word forms and word meaning to see the semantic relations. It is used for wordnets  to get both form and meaning into a computer.  ",
                "Lexical Matrix is a matrix of English lexicon which demonstrates the semantic structure of English vocabulary, by setting different word forms as columns and different word meaning as rows, in order to produce polysemy and synonymy.",
                "Lexical matrix is the mapping between word meaning and word form. When illustrated as a table, each entry is equivalent to a word meaning, if in the row, or to a word form, if in the column. Entries in the same column are considered polysemous and entries in the same row are considered synonyms.",
                "A lexical matrix is a visualisation of lexical semantics, where word form is shown on the columns and word meaning on the rows. Entries in the matrix show that a certain word meaning can be expressed using the word form on top of the column. When a word meaning has multiple entries in a row, it means that the meaning can be expressed using different word forms, thus being synonyms. When a word form has multiple entries in a column, it shows that the word form can have multiple meaning, showing polysemy. A more classical way to express the relation between word form and word meaning is by using the box-and-arrow system. But for faster processing by computer, the lexical matrix was introduced in WordNet."
            ],
            "category": "lexical matrix"
        },
        {
            "num": 6,
            "questions": [
                "word similarity",
                "semantic similarity",
                "What is semantic similarity",
                "What is semantic similarity?",
                "What is the semantic similarity"
            ],
            "responses": [
                "is a method to measure the similarity between two concepts",
                "Semantic similarity is a measure of similarity between words or concepts",
                "Semantic Similarity is how similar words are in meaning. And is the basis for synonymy.",
                "Semantic similarity is a way of expressing the similarity between two or more linguistic items. The semantic similarity describes the level of similarity between two or more concepts.",
                "It consists of different measures that are based on the wordnets. It is an important tool by using natural language processing. For all those measures , the concepts and the place in the taxomony is taken into account. Nevertheless they all differ in their characteristics. ",
                "Two or more words are semantically similar if their meaning or semantic content is similar. Determining how to formalize this rather vague notion is one of the main problems of natural language processing. Nowadays a number of measures of semantic similarity exploit the organization of WordNet. Some methods are: comparing the lengths of paths leading from one concept to another; adding stochastic information adopted from a corpus; comparing concept definitions and glosses; or a combination of some or all of the other methods."
            ],
            "category": "semantic similarity"
        },
        {
            "num": 6,
            "questions": [
                "What is meronymy",
                "What is meronymy?",
                "semantic part hierarchical relation"
            ],
            "responses": [
                "Meronymy describes a relation between words where one has or is part of another.",
                "Meronymy is a part of a hyponym which is considered as a whole. E.g. arms and legs are meronymies of a body.",
                "Meronymy is a hierarchic concept which is based on the whole - part of a whole relation. So x is part of y.  ",
                "Meronymy is a semantic part hierarchical relation between word meanings that identifies an element ‘A’ as part of another ‘B’ element.",
                "This is a part-whole relation, which is the case when something is part of the whole object. For example, a wing is part of a plane, so ‘wing’ is a meronym of ‘plane’, while ‘plane’ is the holonym.",
                "A meronymy can also be considered a partial order. A meronym refers to a part of a whole. A meronym denotes a constituent part of, or a member of something. For example, monitor is a meronym of computer because a monitor is part of a computer."
            ],
            "category": "meronymy"
        },
        {
            "num": 6,
            "questions": [
                "similarity",
                "What is SimLex-999",
                "What is SimLex-999?",
                "similarity, association, semantic evaluation, standard"
            ],
            "responses": [
                "SimLex-999 is a software that measures how well models represent similarity of words",
                "SimLex-999 is a word similarity dataset that generates a value ranging from 0 to 10 that is associated with the similarity",
                "SimLex-999 is a gold standard resource for evaluating distributional semantic models. It evaluates 'similarity' instead of 'assosiation' or 'relatedness' between words.",
                "It is a dataset that contains word pairs and scores the pairs on their similarity. Other datasets often focussed on association only or did not clearly make a distinction between similarity and association.  Each entry also contains the POS-tag for the words.",
                "SimLex-999 is a model which evaluates the performance of distributional semantic models in NLP tasks. It employs similarity measures (e.g. car/bycicle) rather than association measures (e.g. car/petrol), and includes abstract and concrete words, as well as nouns, verbs and adjectives.",
                "SimLex-999 is a standard developed for semantic evaluation of computational models. Unlike many other standards of semantic evaluation, SimLex-999 puts a great emphasis on measuring similarity of terms, not their association strength. This allows for the development of better vector semantic models."
            ],
            "category": "simlex-999"
        },
        {
            "num": 6,
            "questions": [
                "What is precision",
                "What is precision?",
                "What does precision mean?"
            ],
            "responses": [
                "Precision is the number of true positives divided by the sum of the number of true positives and false positives.",
                "Precision is a measure for assessing classifiers which gives the percentage of items that the classifier labeled in accord to human-defined labels",
                "Using an e-mail spam filter as an analogy, the precision would be the number of correctly labeled spam e-mails as a percetage of the total number of received spam e-mails.",
                "Precision is an evaluation metric that measures the percentage of the items that the program identified as positive that were indeed positive according to the human standard. It can be calculated as follows: true positives / (true positives + false positives)",
                "Precision is a score that measures the percentage of items a classifier system correctly detected. It is calculated by dividing the true positives by the sum of the true positives and the false positives. It is also referred to as the Positive Predicted Value.",
                "Precision is a type of accuracy measure. It measures the percentage of the outcomes that is correctly labeled as positive for a measuring system against all the outcomes that are labeled positive. It is often a artificial system versus a human measuring system."
            ],
            "category": "precision"
        },
        {
            "num": 5,
            "questions": [
                "What is embodiment",
                "What is cognitive embodiment"
            ],
            "responses": [
                "Embodiment is some sort of physical form of an intelligent agent.",
                "Cognitive embodiment is the notion that the body of a being (i.e., sensory and motor information) is involved in all kinds of cognitive processes, such as the understanding of word meaning.",
                "The hypothesis that cognitive processes of all kinds are rooted in perception and action. Embodiment means that the semantic information is depended on sensory-motor systems. Unembodied would therefore mean that the semantic content is completely symbolic.",
                "It is the question whether sensory and motor information is a necessary part of semantic representation and processing. The embodiment hypothesis states that cognitive processes are grounded in perception and action. Objectification of semantic representation. ",
                "Embodiment describes the degree to which sensorimotor input influences cognition. Should having a body that allows for human-like sensorimotor input be a prerequisite for human-like cognition, than that should be taken into account when attempting to build an AI that mimics human behavior."
            ],
            "category": "embodiment"
        },
        {
            "num": 5,
            "questions": [
                "computer science",
                "What is Machine Learning",
                "What is machine learning?"
            ],
            "responses": [
                "Machine learning is the study and development of automatic learning algorithms.",
                "When the machine learns from its experiences and acts based on computational models and algorithms and not based on explicit programming.",
                "Machine learning aims to make computers, through computer algorithms, to ‘learn’ how to solve certain problems although they are not programmed to do, provided that ‘their experience’ or the amount of input increases.",
                "Machine learning is to let a machine learn as how human learn.Instead of providing data and rules to the computer and let it generate the result, machine learning provides data and result to the computer and asks the computer to figure out the rules.",
                "Machine learning is a subfield of Artificial Intelligence concerning computer algorithms that are automatically enhanced through experience. Its goal is to develop computers that can learn every task humans can perform without being explicitly pre-programmed."
            ],
            "category": "machine learning"
        },
        {
            "num": 5,
            "questions": [
                "synset",
                "What are synsets",
                "What is a synset"
            ],
            "responses": [
                "Group of words that are interchangeable synonyms.",
                "Synset is a set of synonyms we find in WordNet. They represent word meanings",
                "A synset, short for synonym set, is a set of words that have approximately the same meaning",
                "Synsets are synonym pair sets employed in a lexical matrix to represent word meanings. Since they enable us to differentiate a meaning from others, they signal that the word exists.",
                "In WordNet, all words are grouped into synsets. Each synset denotes a certain concept which can be of any degree of specificity, and it contains all words that are more or less synonymous with that concept and, per extension, with one another."
            ],
            "category": "synset"
        },
        {
            "num": 5,
            "questions": [
                "lexical relation",
                "What is antonymy",
                " What is antonymy?",
                "What is vector antonymy?"
            ],
            "responses": [
                "A word has an antonymous relationship with words that are considered its opposites, such as weak and strong or rich and poor.",
                "Antonymy is the lexical relation between word forms that identifies the opposite of a word. On WordNet is mostly applied to adverbs and adjectives.",
                "Antonymy is a lexical relation of non-similarity between word forms that provides a central organizing principle for adjectives and adverbs in Wordnet",
                "Antonymy is when two lemmas are semantically opposed. This is the lexical relation between word forms and not word meanings. In WordNet, this is a principle that is central for adjectives and adverbs.",
                "Antonymy refers to the relation between words with opposing meanings, and is a lexical relation between word forms, and not a semantic relation between word meanings. The antonym of a word x is often not-x, but this is not always the case, especially when the words represent polar opposites of a continuum (such as rich and poor)."
            ],
            "category": "antonymy"
        },
        {
            "num": 5,
            "questions": [
                "evaluation",
                "cross-validation",
                "What is Cross-Validation"
            ],
            "responses": [
                "Cross-validation is a technique that combines scores from different evaluations of test sets.",
                "Cross-Validation takes the result from different test sets and combines these test set scores to reevaluate it and to test a new fold",
                "A data set is split randomly into a training set and a test set, where a model analyses the training set with known data, and validates its analysis on the test set of unknown data. This process is usually repeated several times.",
                "Cross-validation is the process of validating a model by dividing the data in an amount of equal parts (folds), for example 5, and training the model on all the parts except one. The model is than tested on that part. This process is repeated until the model is tested on all the parts once, after which the average over all test is calculated.",
                "When training a classifier, we always need a training set, a development set and a test set. The problem we run into then is that we are dividing up our corpus into small chunks, however, it is crucial that all our sets are large enough to still remain representative of the language. A common way of addressing this issue is to randomly divide the data into different training sets and test sets, carry out one instance of a training and then test it on the test set. We repeat this manoeuvre 10 times to result in a '10-fold cross-validation'. It is based on this process that the error rates are calculated for the classifier. Most commonly throughout the process a chunk of the data is kept separate as a test set to make sure the classifier will not overgeneralize to the idiosyncrasies of our data."
            ],
            "category": "cross-validation"
        },
        {
            "num": 5,
            "questions": [
                "class",
                "generative model",
                "what is naive Bayes",
                "What is Naive Bayes?"
            ],
            "responses": [
                "Naive Bayes is a probabilistic machine learning model used for classification tasks.",
                "Naive Bayes is a generative Bayesian classifier that makes a simplifying assumption about how the features interact.",
                "Naive Bayes is a generative classifier which builds a model to return the most possible class to allow the observation to be generated.",
                "Naive Bayes is a conditional probabilistic model that assigns a probability to an instance that has to be classified with a certain label. Naive Bayes classifiers with binariezed features usually work better for the majority of classification tasks.",
                "Naive Bayes is an analysis classifier system. It is a relatively simple system which assigns class to words based on prior probability and likelihood. It is a so called naive system because its assumption of probabilities are independent of class and on how features interact. The positions of words in a sentence are ignored and there is made use of the frequency of words instead."
            ],
            "category": "naive bayes"
        },
        {
            "num": 4,
            "questions": [
                "What is turing test",
                "What is Turning-test:",
                "What is the Turing Test",
                "What is the Imitation Game"
            ],
            "responses": [
                "is a test done by human in order to check if the human can figure out if he/she speaking to human or machine. I like this term because if we can reach that level of human-like conversation, then it will be a turning point for the robotics market.",
                "Turing test is a test coined in the 1950's by Alan M. Turing. It is suggested that a computer is close to having a human mind or passing as a human if it passes the Turing test. In this test a computer succeeds if it fools an expert that it is a human giving the responses",
                "In the Turing Test, a human judge has a conversation with a machine in which the human can ask any question. If the machine tricks the judge into thinking that it is human, the machine has passed the Turing Test, which would imply that the machine is at a level of intelligence comparable to that of a human",
                "In 1950 Alan Turing proposed a game that should replace the question of Can machines think?. Because this question relies to heavily on the definitions of machine and think, The Imitation Game, as Turing called it, was meant as a more practical solution. In short, the game hosts 2 humans and a machine. One of the humans can interrogate both the machine and other human, without directly seeing (or hearing) them. The goal is for the machine to convince the interrogator, that it is the human, while the human must convince the interrogator, that the machine is the machine."
            ],
            "category": "turing-test"
        },
        {
            "num": 4,
            "questions": [
                "ambiguity",
                "What is ambiguity",
                "What is ambiguity?"
            ],
            "responses": [
                "Ambiguity in Speech and Language Processing occurs when the syntax of linguistic units is capable of more than one interpretation.",
                "Ambiguity occurs when a unit of human language can be interpreted in more than one way. Ambiguity can be on any linguistic level (phonology, morphology, syntax, semantics, pragmatics, discourse). A very important task of Speech and Language Processing is locating where a piece of language has an ambiguous aspect, to then figure out the most likely interpretation.",
                "There are more linguistic structures possible for a sentence. Algorithms are used to resolve these ambiguities. There are different types of ambiguity, namely part of speech tagging: Is something used as a verb or a noun. Word sense disambiguation: a word can represent all other kind of words. Probabilistic parsing: does two entities belong to each other or are those different kind of entities.",
                " In linguistics when referring to the phenomenon of ambiguity, a part of written text or speech is open to multiple interpretations. Lexical ambiguity makes it often difficult for humans to understand the meaning of the specific part of text. When talking about AI, ambiguity can form a big challenge for an artificial agent to convey the meaning of the text. Ambiguity often occurs because elements in a sentence carry different meanings, this form of ambiguity is called lexical ambiguity. Another way in which language can be ambiguous is described by the term “structural ambiguity”, which means that ambiguity arises as a result of word order or order of different phrases. "
            ],
            "category": "ambiguity"
        },
        {
            "num": 4,
            "questions": [
                "NLP",
                "NPL",
                "language processing",
                "speech and language processing",
                "What is speech and language processing",
                "What is Speech and Language Processing?",
                "How is Speech and Language Processing defined?"
            ],
            "responses": [
                "Speech and language processing is a field that uses computational methods to process human written and spoken language",
                "Speech and Language Processing is the study of the techniques adopted to process language and speech in computer science.",
                "Speech and Language Processing is the common field of NLP, Computational Linguistics, Speech Recognition etc. In other words, it is the common field of all the language processing applications that possess the 'understanding' of language.",
                "Speech and Language Processing is an umbrella term for the fields of Natural Language Processing, Computational Linguistics and Speech Recognition and Synthesis. These fields share the goal of finding out how human language can be dealt with by machines or artificial agents."
            ],
            "category": "speech and language processing"
        },
        {
            "num": 4,
            "questions": [
                "weak AI",
                "What is Weak AI?"
            ],
            "responses": [
                "An approach that supports the idea that computer models are useful in studying the mind, but is not completely similar.",
                "Weak AI presupposes that thinking and other cognitive processes go beyond its formal structure, thus they cannot be duplicated by a computer. Computers simulations of certain functions of the human mind may merely be an interesting method to study the formal aspect of those cognitive functions.",
                "A weak AI is a machine intelligence built either for a specific purpose or built by modelling parts of human cognition. As such, the creators of weak AI’s do not claim that weak AI’s replicate human cognition in its entirety. Their value, however, lies in either carrying out their specialised task or by providing valuable insight into how the human mind might work.",
                "Weak Artificial Intelligence is the counterpart of Strong Artificial Intelligence. It claims that the output of humans and machines can be the same, i.e. the responses that they give on questions, but that the underlying processes do not necessarily have to be the same. It says that the claim that machine processing is the same as how the human brain works is a too strong claim. Human thinking can be compared to manipulation of symbols, but it also deals with emotions and word meaning. Executing the syntax is not the same as also interpreting the semantics and pragmatics in a conversation. "
            ],
            "category": "weak ai"
        },
        {
            "num": 4,
            "questions": [
                "semantic relations",
                "What are semantic relations",
                "What is a Semantic Relation?",
                "What are semantic relations?"
            ],
            "responses": [
                "Used to create WordNet. Besides synonyms the following relations are also used: anatomy (conceptual opposites), hyponymy (is a type of), meronym, morphological relations.",
                "A semantic relation between meanings. Semantic relations can be broader dan semantic similarity. For instance, bike is in a semantic relation with vehicles, but is not similar to vehicles.",
                "Semantic relations are the relations in meaning between different words. These relations occur within the same word categories. A few examples of semantic relations are synonymy, antonymy, homonymy and membership. To express meaning, WordNet makes use of so called synsets, sets of two synonyms, that signify the existence of a concept. WordNet proposes that the semantic relations can be shown as pointers between synsets.",
                "The main organizational principle of WordNet is semantic relations between word senses. Some nouns, for instance, represent a subset of a concept. Crow is a hyponym of bird as crows are a type of bird. Beak, on the other hand, is a meronym of bird as beaks are a ‘part’ of a bird. These semantic relations are reflected by the WordNet network. Other word classes have different and separate organizational principles, verbs, for instance, are organized by entailment relations. Eat, for instance, entails chew (under normal circumstances), thus the two verbs are semantically related in WordNet."
            ],
            "category": "semantic relations"
        },
        {
            "num": 4,
            "questions": [
                "What is lexical matrix",
                "What is a lexical matrix",
                "What is a lexical matrix?",
                "How can we represent the realtion between meaning and form?"
            ],
            "responses": [
                "A lexical matrix shows the relationship between word forms and word meanings. Word meanings serve as headings for each row in the matrix, while word forms serve as headings for each column. An entry in the matrix can then be described as a mapping between a word form and a word meaning. ",
                "A way to represent a lexicon as a relation between word meanings and word forms, where a word meaning can be represented by a set of synonyms. On the top horizontally are the word forms and, on the left, vertically are the word meanings. If the same meaning has two forms, those two forms are synonyms.",
                "A lexical matrix is a way to order word meaning and word and word form in a neat way. It represents the mapping between written words and synsets (a set of synonyms). This way, one can easily see multiple meanings of a word form, or induce the meaning of a word by looking at different word form with the same meaning (assuming knowledge of the language).",
                "A lexical matrix is a way of mapping between a word form and a word meaning. In this matrix, word forms are the column headings and word meanings are the row headings. An entry in a cell that maps between a word form and a meaning means that the word form in that column can be used to express the meaning in that row.  For a given word form or a given meaning there could be multiple mappings"
            ],
            "category": "lexical-matrix"
        },
        {
            "num": 4,
            "questions": [
                "Cosine",
                "Cosine Similarity",
                "What is cosine similarity",
                "What is cosine similarity?"
            ],
            "responses": [
                "a metric used to see how similar two (word) vectors are.",
                "Cosine similarity is a metric used to measure how similar the objects are regardless of their size. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space.",
                "Cosine similarity measures the similarity between two vectors of an inner product space, it can be used to is often used to measure document or word similarity. The angle is calculated between two representations in a multidimensional space.",
                "In mathematics, cosine is one of the trigonometric functions and describes the relationship between the adjacent side and the hypotenuse side of a right-angled triangle (adjacent/hypotenuse). The cosine similarity is a value between two vectors that is used to describe the similarity between these two vectors. This value is the calculated cosine of the angle of two vectors. The cosine similarity is used to calculate and describe the similarity between two objects, in this case semantic entities. The more similarity, the closer the value of cosine similarity will be to 1. Words that have a totally different orientation (oriented at 90 degrees), have a similarity value of 0. "
            ],
            "category": "cosine similarity"
        },
        {
            "num": 4,
            "questions": [
                "What is distributional semantics",
                "What is distributional semantics?",
                "distributional semantics, statistical distribution, corpus-based, usage-based, word co-occurence"
            ],
            "responses": [
                "The study of word meaning based on word co-occurence statistics in large data samples.",
                "Distributional semantics is an approach to word meaning that measures the similarity of words based on the contexts in which they appear.",
                "Distributional semantics is based on the theory that words which are used in the same context, possibly have similar meanings. This relation (in semantic spaces) is a symmetric relation: if a word is close to another word in a certain semantic space, it also works the other way around. Distributinal Hypothesis: the level of similarity concers semantics between two linguistic outing is a outcome of the similarity of the (linguistic) contexts in which the two linguistic elements appear. ",
                "Distributional semantics is the idea that semantics of words is determined by the context of these words. The degree in which the meaning is determined by this context can vary; it can partly be determined by the definition of the word and partly by the context (soft distributional semantics) or it can solely be determined by the context (hard destributional semantics). Ususally, the distributional semantics of a word is represented by a vector that contains whether or not the word appears near other words."
            ],
            "category": "distributional semantics"
        },
        {
            "num": 4,
            "questions": [
                "What is embedding",
                "What is embedding?",
                "What are embeddings?"
            ],
            "responses": [
                "Embedding is the representation of a word as a vector in a vector space",
                "Embedding is the process of calculating the value of a word for a specific factor space",
                "In linguistics, word embeddings are language models where words and phrases are being mapped to vectors or numbers.",
                "Embeddings are are representations of words in a dimensional space (vectors). Through embeddings, sentiment analysis can be performed. Also, because of the dimensional representation it shows word similarity. "
            ],
            "category": "embedding"
        },
        {
            "num": 4,
            "questions": [
                "What is tf-idk",
                "What is tf-idf",
                "What is tf-idk model",
                "What is tf-idk algorithm"
            ],
            "responses": [
                "Term frequency-Inverse document frequency. A calculation can be made to calculate the similarity between documents.",
                "tf-idf stands for term frequency-inverse document frequency, and it is a number that indicates how typical a word is for a text in a given corpus. It is low for words that don’t occur often in a text, and for words that do occur often in a text, but also often in many other texts in the same corpus. It is higher for words that occur often in a text, but not often in most other texts.",
                "A way of weighing words. It is based on the idea that term frequency alone is not informative enough. Words llike ‘a’ and ‘and’, appear many times in every text, and therefore add nothing to the meaning of the text. Therefore, there is also room for the amount of documents the term appears in. If this number is high, the information value is low, and vice versa. The tf-idf is thus: (frequency of the term in the document) / (nr of documents the term appears in)",
                "Tf-idf stands for term frequency-Inverse document frequency. This algorithm is used to weigh how important words are to documents. This way important words that tell us something important about the context can be utilized while frequent words which appear in every document can be omitted. A word is important when its term frequency in a specific document is high while its frequency in all the collection of documents is relatively low. This means that the word is more important for a specific context."
            ],
            "category": "tf-idf"
        },
        {
            "num": 4,
            "questions": [
                "What are embeddings",
                "What are embeddings?",
                "What are Word Vectors",
                "What are Word Embeddings"
            ],
            "responses": [
                "Embeddings are the representation of words based on their distribution in the text.",
                "Embeddings are the vectors that represent a certain word as a point in a semantic space.",
                "Embeddings is what vectors that represent words are generally called. Each dimension of such a vector is the frequency of the word in a given context.",
                "Word Vectors (or Word Embeddings) are points in a multi-dimensional space represented by real numbers, where each number describes a different aspect of the word's meaning (i.e., a word's meaning is defined by its location compared to other words in the space)"
            ],
            "category": "embeddings"
        },
        {
            "num": 4,
            "questions": [
                "What is sentiment analysis",
                "What is sentiment-analysis",
                "What is sentiment analysis?",
                "What does sentiment analysis mean?"
            ],
            "responses": [
                "Sentiment analysis is the classification of text based on subjective information, for example, a text can be classified as positive, negative or neutral",
                "Sentiment anlysis is classifying the sentiment of a text, the attitude and feelings the author is expressing, positive or negative, through analysis of the vocabulary .",
                "Sentiment analysis is assigning a positive or negative sentiment to a text, based on the author's opinions. For example, sentiment analysis can be applied on movie reviews or product reviews.",
                "Sentiment analysis is a classification task that focusses on the sentiment in the input. Often, this comes down to deciding wether the input is positive or negative, but it can also be more advanced."
            ],
            "category": "sentiment-analysis"
        },
        {
            "num": 3,
            "questions": [
                "AI",
                "machine learning",
                "What is Artificial Intelligence",
                "What is Artificial Intelligence?",
                "what is Artificial intelligence from neuroscience point of view",
                "what is Artificial intelligence from philosophical point of view"
            ],
            "responses": [
                "Artificial Intelligence is the development of humanlike intelligence for machines with the purpose of making them able to perform human’s tasks.",
                "The term refers to the ability of 'machines' to 'think'. In order to perceive this clearly, we need to specify our criteria about what is a 'machine' and what does 'think' means.",
                "Artificial intelligence is: from philosophical point of view in contrary with human natural intelligence, unnatural intelligence which human beings-considering themselves intelligent- grant machines through manipulating programs to make machines generate man-kind ‘reaction’; from neuroscience point of view, a technology to make computers imitate how human brain works in order to let computers react as how humans do."
            ],
            "category": "artificial intelligence"
        },
        {
            "num": 3,
            "questions": [
                "Compositionality",
                "What is compositionality?",
                "Principle of compositionality",
                "What does compositionality mean?"
            ],
            "responses": [
                "The meaning of a complex expression is determined by the meanings of its constituent expressions and the rules used to combine them",
                "Compositionality means that the meaning of a bigger phrase is being determined by the meanings of the smaller components of the phrase.",
                "The principle of compositionality states that the meaning of a lexical entity or phrase is defined by the meaning of its components. In computer science, the principle of compositionality plays a key role in denotational semantics of programming languages. This approach is used to formalize the meanings of a computer language by describing the meanings of the expressions used to construct this language. A problem that is often discussed regarding the principle of compositionality is that this principle cannot distinguish literal predications from metaphors. Furthermore, compositionality cannot give an adequate description of sentences that carry  different meaning when words or phrases are ordered in a different order. "
            ],
            "category": "compositionality"
        },
        {
            "num": 3,
            "questions": [
                "What is bag-of-words model",
                "What is the bag-of-words model ",
                "what is neglected in bag-of-words model"
            ],
            "responses": [
                "a model that represents text as a set of words without taking word order into consideration. One-gram models are also BoW models. ",
                "Bag-of-words model is a traditional approach of NLP which is used to capture the frequency of words by considering the observed document as a bag of words and neglecting the syntax of the observation.",
                "The bag-of-words model is a way of representing texts as if all words are in a large bag. It ignores the order of the words and usually the grammer but remembers the frequencies of the words in a text, commonly with a dictionary. This makes it easier and more efficient to process the whole text and classify it. The words and their frequencies can be used a features in the feature vector."
            ],
            "category": "bag-of-words model"
        },
        {
            "num": 3,
            "questions": [
                "Similarity",
                "What is similarity?",
                "What is semantic similarity",
                "What is semantic association"
            ],
            "responses": [
                "The extent to which words share features such as physical, common function or category",
                "Words that share common properties, such as 'cup' and 'mug', are semantically similar, while words that often occur in the same context, such as 'coffee' and 'cup', are semantically associated",
                "Similarity in semantics is for instance the case with specific vehicles. A car is similar to a bike, but not exactly the same. The two elements (bike and car) share certain properties: they are both used for the same goal (travelling from the one place to another place) but aren't exactly the same (a bike has two wheels, a car has four wheels). "
            ],
            "category": "similarity"
        },
        {
            "num": 3,
            "questions": [
                "What is distributional semantics",
                "What is distributional semantics?",
                "What does distributional semantics mean?"
            ],
            "responses": [
                "Distributional semantics is a way to analyze semantic similarities, by studying linguistic items and their distribution in large samples of language data",
                "Distributional semantics is a field which uses different approaches to semantics that assumes that the statistical distibution of words and the context they appear in can hep us identify their meaning.",
                "In distributional semantics, a correlation between a word’s meaning and its distribution in texts is drawn. Under this assumption, the meaning of a word relative to the meanings of other words can more or less be extracted from the contexts in which it appears."
            ],
            "category": "distributional-semantics"
        },
        {
            "num": 3,
            "questions": [
                "What are word vectors",
                "What is vector semantics",
                "What are word embeddings",
                "What is vector semantics?",
                "What does vector semantics mean?"
            ],
            "responses": [
                "Vector semantics is a model that represents word meaning as point in space, applying the distribution hypothesis by learning representations of word meaning directly from their distributions in texts",
                "Vector semantics' goal is to represent a word as a point in a multidimensional space. A vector (or an embedding) is a list of numbers that defines the word based on the numbers of neighbouring words, but how this is calculated varies per method.",
                "Vector semantics is when words are represented as points into a multidimensional semantic space. The distance between the words (embeddings) represents some aspect of relatedness between the words. For example, words that appear in similar contexts might appear closer together than completely unrelated words."
            ],
            "category": "vector-semantics"
        },
        {
            "num": 3,
            "questions": [
                "What is semantic space?",
                "what is a semantic space",
                "semantic space, distributional space"
            ],
            "responses": [
                "Semantic space is the space around a vector. Words with a short distance to a vector are often stronger semantically related to the vector word than words with a longer distance. The semantic space is the space wherein semantically related words occur.",
                "A semantic space is a space whose dimensions are provided by the relevant linguistic contexts, and in which the position of a word-vector is determined by its statistical distribution in each context. Distances between vectors correspond to semantic distances between the corresponding words.",
                "A space whose dimensions are determined by the relevant linguistic context and where word meanings are represented as coordinate points, defined according to their distributional probabilities in each context. Words that are closer to each other in this space also have closely related meanings."
            ],
            "category": "semantic space"
        },
        {
            "num": 3,
            "questions": [
                "What is Overfitting",
                "What is overfitting?",
                "What does ovefitting mean?"
            ],
            "responses": [
                "Overfitting is a problem in machine learning when a model isn't generalisable and to much specified for the training data.",
                "Overfitting means providing your algorithm with so many features that your model doesn't generalize well when providing new data.",
                "Overfitting is when there are too many features in a given learning algorithm, for example, when a feature extractor overfits Gender Features by assigning it to the first letter of a name, the last letter of a name, count, and applying lower case to that name"
            ],
            "category": "overfitting"
        },
        {
            "num": 3,
            "questions": [
                "What are naive Bayes classifiers?",
                "What are naive bayes classifiers?",
                "What is a naive Bayes classifier?"
            ],
            "responses": [
                "Naive Bayes classifiers are probabilistic classifiers based on applying Bayes' theorem with strongindependence assumptions between features.",
                "A Bayes classifier is a probabilistic classifier. A probabilistic classifier is used to tell the probability of the input observation in a class. Additionally, a Bayes classifier makes a naïve presumption about the interaction of certain features. This presumption is a simplifying one.",
                "It makes an assumption how features interact. This assumption is based on prior probability of each label. Meaning that each label's frequency is measured within the training set. The combination of prior probability and frequency are than combined to get a likelihood estimate for each label. This is most often used for sentiment analysis, since frequency of positive or negative words are most important here."
            ],
            "category": "naive bayes classifiers"
        },
        {
            "num": 3,
            "questions": [
                "What is text categorization",
                "What are the most common text categorizations",
                "What does text categorization mean and how is it used?"
            ],
            "responses": [
                "Text categorization is to assign a category to an input. The most common text categorizations are sentiment analysis,Spam detection,language id and authorship attribution.",
                "Text categorization assigns a label to an entire text. Examples are sentiment analysis that can see whether a text has a positive or negative orientation. Also there are subject categories which classify a text to a certain subject category or topic label.  ",
                "Text categorization is the task of labeling texts and thus putting them in a category. Concrete examples of text categorization are sentiment analysis (analyzing the orientation that the writer has towards their text), spam detection (classifying emails as 'spam' or 'no spam') and language detection (detecting the language a text was written in). A common way to perform text categorization is with supervised machine learning. A number of texts are then given to the machine along with the labels of their correct category. The machine will adjust its paramters in a way that the texts are categorized in the correct label, so that new texts, for which the machine does not have a label, are categorized correctly."
            ],
            "category": "text categorization"
        },
        {
            "num": 3,
            "questions": [
                "metrics",
                "contingency table",
                "What are precision and recall",
                "what are precision and recall?"
            ],
            "responses": [
                " Whenever there is an unbalanced situation in frequency it is better to use precision and recall. Precision measures the items that are indeed detected and were positive. Recall measures the percentage of items that were correctly identified by the system.",
                "Precision and recall are two important metrics that are taken into consideration in a contingency table. Precision measures the percentage of the real positive items in a system. Recall measures the percentage of all the correctly identified items among the ones actually present.",
                "Calculations made to evaluate a classifier. This is done by calculating the precentage of items that we labeled were indeed this label (labeled correctly /(correctly labeled + falsely labeled). Recall is the percentage of correctly labeled items is in comparising to the total of that particular label is (labeled correctly/(labled correctly + not labeled)"
            ],
            "category": "precision and recall"
        },
        {
            "num": 3,
            "questions": [
                "What is naive Bayes",
                "What is a Naive Bayes classifier",
                "What is naive Bayes classification"
            ],
            "responses": [
                "Naive Bayes is a classifier. What makes it naive is the fact that is uses the bag-of-words representation, which does not take word order into account and  the naive Bayes assumption.",
                "If Bayes’ theorem is used in a classification text, where the probability of a class given a document as a set of features is calculated, a naive Bayes classifier (naively) assumes no relation between the features that make up the document, making direct computation of the probability possible.",
                "A Naive Bayes classifier is a probabilistic classifier (i.e., its classifications are based on probabilites/likelihoods) that is based on two assumptions: language is represented as a bag-of-words (word position does not matter) and words are conditionally independent of each other given the class"
            ],
            "category": "naive-bayes"
        },
        {
            "num": 2,
            "questions": [
                "What is weak AI",
                "What is strong AI"
            ],
            "responses": [
                "Strong AI is an approach to AI in which computer programs simulating human cognition not only model the human mind but are actually a mind",
                "John R. Searle divided AI into two types: strong and weak AI. Strong AI, would be AI that is similar to the intelligence of humans, in that it has a consciousness. Nowadays this form of AI is better known under the term General Artificial Intelligence or GAI. In contrast to strong AI, weak AI is limited in what it can do."
            ],
            "category": "strong-ai"
        },
        {
            "num": 2,
            "questions": [
                "What is the Chinese Room",
                "What is the Chinese Room argument"
            ],
            "responses": [
                "The Chinese Room Argument is a thought experiment in which a person, sitting in a closed-off room, translates Chinese input sentences solely by using a rulebook, without understanding the meaning of the sentences. According to John Searle, this is how machines 'think': they formally manipulate symbols without understanding the semantics",
                "In order to prove that a machine could never have a conscious mind, Searle came up with an analogy, called the Chinese room. Suppose a room full of people, that have all resources that they need in order to reply in Chinese to a Chinese question (both written). Then by following all the rules of their resources, the room would be able to produce an answer, without ever having to understand both the question  and the reply. This would prove, that a machine can’t attach meaning to the symbols, but rather just carries out a set of instructions."
            ],
            "category": "chinese-room"
        },
        {
            "num": 2,
            "questions": [
                "What is speech and language processing",
                "What is speech-and-language processing"
            ],
            "responses": [
                "Speech and language processing is an umbrella term that encompasses Natural Language processing (NLP), Computational Linguistics and Speech Recognition and Synthesis",
                "a combination of the fields of Natural Language Processing, Computational Linguistics and Speech Recognition and Synthesis. It would be ably to interact with humans using natural language, including written and spoken language. To make this possible, knowledge of phonetics and phonology, morphology, syntax, semantics, pragmatics and discourse would be required."
            ],
            "category": "speech-and-language-processing"
        },
        {
            "num": 2,
            "questions": [
                "What is a digital computer",
                "What is a digital computer?"
            ],
            "responses": [
                "Machine sufficient storage capacity, processing power (executive unit) and control to be able to read and perform tasks correctly until some condition is fulfilled",
                "TA machine that consists of three parts: Store, Executive Unit and Control. The store consists of information and can be compared to a paper which is used by a human-computer while working on a calculation for example. The executive unit is in charge of carrying out calculation-based individual operations. The control has to look over whether instructions are followed and if they are done in the right order."
            ],
            "category": "digital computer"
        },
        {
            "num": 2,
            "questions": [
                "What is semantics",
                "What is semnatics"
            ],
            "responses": [
                "Attaching meaning and context to words and symbols",
                "Semantics is the field of linguistics that researches the meaning of words, phrases, sentences and complete texts."
            ],
            "category": "semantics"
        },
        {
            "num": 2,
            "questions": [
                "What is Chinese room experiment",
                "What is the Chinese room experiment"
            ],
            "responses": [
                "A hypothetical setting where you put someone that doesn’t know Chinese in a room with a complete guide how to use the Chinese language system. Although this person is able to communicate in Chinese (may it be a bit slow), it doesn’t show he/she understands the language. The person in the room in representative for a computer that communicates like a human and therefor, even if such computers exist, it cannot prove that the computer thinks like a human.",
                "The Chinese Room argument was first published in 1980 by American philosopher John Searle. It has become one of the best-known arguments in recent philosophy. Searle imagines himself alone in a room following a computer program for responding to Chinese characters slipped under the door. Searle understands nothing of Chinese, and yet, by following the program for manipulating symbols and numerals just as a computer does, he sends appropriate strings of Chinese characters back out under the door, and this leads those outside to mistakenly suppose there is a Chinese speaker in the room."
            ],
            "category": "chinese room experiment"
        },
        {
            "num": 2,
            "questions": [
                "What is a confusion matrix?"
            ],
            "responses": [
                "It is a matrix that gives information about errors made by the model. It shows how often the prediction of a model was right or wrong. This is done by true positives: relevant items that were correctly identified. True negatives: irrelevant items that were correctly identified. False positives (type 1 error): irrelevant items that were incorrectly identified. False negatives (Type 2 error) relevant items that were incorrectly identified."
            ],
            "category": "confusion matrix"
        },
        {
            "num": 2,
            "questions": [
                "what is semantic embodiment"
            ],
            "responses": [
                "Semantic embodiment is the representation of semantic content in the form of motor and sensory information.",
                "Semantic embodiment is the theory that sensory and motor processes are influencing the semantic understanding of words. When a person has knowledge about an action activity is shown in the motor neurons in the brain when perceiving text about these activities. This is a strong embodiment. If only sections of the brain around the primary region activate, it is called weak embodiment. Lesions in the brain in areas where motor neurons are present relate to a worse understanding of the semantics of actions, for example kicking a ball."
            ],
            "category": "semantic embodiment"
        },
        {
            "num": 2,
            "questions": [
                "What is Semantic representation",
                "What is semantic representation?"
            ],
            "responses": [
                "Abstract, formal way in which the meaning of sentences can be presented. ",
                "Semantic representation is a method used to represent the meaning of one single word through examining the relationship among other words and analyzing the collection of all possible meanings of this word."
            ],
            "category": "semantic representation"
        },
        {
            "num": 2,
            "questions": [
                "What is Embodiment hypothesis",
                "What are embodiment hypothesis?"
            ],
            "responses": [
                "Embodiment hypothesis is if cognitive embodiment plays an essential role in semantic representation and processing, given the hypothesis that perception(sensory) and action(motor) are the basis of all types of cognitive processes.",
                "This is whether sensory and motor information is actually needed in processing and semantic representation. Unembodied theories claim that in semantic representation, there is no role for sensory and motor information as semantic information is symbolic. Secondary embodiment proposes that semantic content is amodal (modality invariant), but there is no strict boundary between it and sensory and modal information. Weak embodiment claims that sensory and motor information are partly needed for semantic representation, which is the case if semantic processing has a representational role. Strong embodiment claims that semantic content is represented by both the sensory and motor regions via simulation."
            ],
            "category": "embodiment hypothesis"
        },
        {
            "num": 2,
            "questions": [
                "What is transduction",
                "What is transduction?"
            ],
            "responses": [
                "Transduction is the translation between different types of information.",
                "Transforming from one information type (signal) to another information type (symbol). Cognitive processes can manipulate it."
            ],
            "category": "transduction"
        },
        {
            "num": 2,
            "questions": [
                "lexical relations",
                "What is Lexical Relations"
            ],
            "responses": [
                "Lexical relations between word forms are for example, synonyms (different word with similar meaning) and antonyms (opposite meaning) of words, whereas lexical relations between word meanings are hyponymy/hypernymy (denotes a subcategory, e.g. ‘bed’ and ‘furniture’ ), and meronymy (part-whole relation), for example, “more hands are needed for this project.”; In this example ‘hands’ refer to  ‘people’.",
                ":Besides semantic relations, WordNet also represents the relation between word forms. Two word forms represent the same word sense when they are synonyms: rug and carpet both mean the fuzzy item under our feet in a room. Synonyms are an important organizational principle for nouns. Antonyms, on the other hand, are words that conventionally express opposite senses. Rich and poor are antonyms, but affluent and poor are not so saliently so, even though affluent is a synonym of rich."
            ],
            "category": "lexical relations"
        },
        {
            "num": 2,
            "questions": [
                "What are Semantic Similarity measures?",
                "What is a Semantic Similarity Measure?"
            ],
            "responses": [
                "They are methods to define approximation in meaning between words.",
                "Semantic Similarity Measures are methods used for measuring semantic similarity between two words, most often nouns. Semantic Similarity is used for different purposes within linguistic research, but also psychology or cognitive science. It can be used for natural language processing, word sense disambiguation or text segmentation. The four most used measures are path length based measures, information content based measures, feature based measures and hybrid measures."
            ],
            "category": "semantic similarity measures"
        },
        {
            "num": 2,
            "questions": [
                "What is word sense disambiguation?"
            ],
            "responses": [
                "Word sense disambiguation is the task of selecting the correct sense of a word, mostly based on a context.",
                "Word sense disambiguation (WSD) is the task of figuring out the correct sense of a word. Two WSD task are highlighted: the lexical sample task and the all-word task. The first uses a relatively small set of pre-selected words, while the second deals with entire texts that are more representative of data from ‘the real world’."
            ],
            "category": "word sense disambiguation"
        },
        {
            "num": 2,
            "questions": [
                "what is meronomy",
                "What is meronomy? "
            ],
            "responses": [
                "A concept x is a meronym of a concept y if an x is a part of y.",
                "Meronomy refers to the relation where one word (or sense) is a part of another word. For example, a leaf is a part of a branch, and a branch is a part of a tree, and a tree is a part of a forest."
            ],
            "category": "meronomy"
        },
        {
            "num": 2,
            "questions": [
                "What is zipf's law",
                "What is Zipf’s law"
            ],
            "responses": [
                "Zipf's Law is used to describe the relationship between word frequencies over a corpus or text. It states that if all the words in a text are ordered by frequency, the word frequency is inversely proportional to its rand i.e. the second most frequently occurring word will be half that of the first and so on.",
                "The law takes the name from American linguist George Kingsley Zipf who proposed the theory. According to Zipf, the relative frequency of a word is inversely proportional to its rank. To put it another way, the frequency distribution of words defines the rank. The probability can be calculated conforming to the rank."
            ],
            "category": "zipf’s law"
        },
        {
            "num": 2,
            "questions": [
                "What is Word Sense",
                "What is a word sense"
            ],
            "responses": [
                "Word sense is the different meanings of a word, for example, 'cell' can mean 'a cell in your body' or 'a jail cell' or 'a cell phone'",
                "A word sense is a specific meaning of a word, that is distinctive from other meanings of the same word. Most words have multiple senses."
            ],
            "category": "word sense"
        },
        {
            "num": 2,
            "questions": [
                "What is a semantic relation"
            ],
            "responses": [
                "A semantic relation is a relation between word meanings, such as hyponymy and meronymy",
                "A semantic relation is the relation between lexical concepts in terms of synonymy, antonymy, hyponymy and meronymy"
            ],
            "category": "semantic-relation"
        },
        {
            "num": 2,
            "questions": [
                "semantics",
                "word meaning",
                "What is word meaning"
            ],
            "responses": [
                "Word meaning refers to the semantic content of word forms; what a string of sounds refer to; the main object of study of Semantics.",
                "The study of word meaning is called lexical semantics. This entails some different areas: word senses (synonym/ antonyms etc), word similarity and relateness (coffee and tea are somewhat similar coffee and cup are somewhat related, semantic frames an roles (Perspectives/Participant in a particular type of event), taxonomic relations (superordinate = hypernym / subordinate = hyponym), connotation, etc."
            ],
            "category": "word meaning"
        },
        {
            "num": 2,
            "questions": [
                "What is a vector"
            ],
            "responses": [
                "A vector is the calculated value of a word",
                "A vector is a sequence of numbers that encode the statistical association strength between a word and a certain context or distributional feature. "
            ],
            "category": "vector"
        },
        {
            "num": 2,
            "questions": [
                "What is latent semantic analysis"
            ],
            "responses": [
                " is a method used in topic classification",
                "a BoW model that outputs term-document matrix on which latent values are learned using singular value decomposition."
            ],
            "category": "latent semantic analysis"
        },
        {
            "num": 2,
            "questions": [
                "What is a word2vec model?",
                "What is the word2vec model?"
            ],
            "responses": [
                " Word2Vec is a vector representation that is shorter and denser than the standard vector representation.It contains information about how likely the target word is to appear next to another word.",
                "A word2vec model represents a word in short and dense vectors. Instead of counting co-occurrances, this model uses binary classification to indicate how likely one word is to occur next to another."
            ],
            "category": "word2vec model"
        },
        {
            "num": 2,
            "questions": [
                "What is word analogy"
            ],
            "responses": [
                " link words which usually are related or mostly will be found in the same context",
                "Analogy is a cognitive process of transferring information or meaning from a particular subject to another. The term analogy can also refer to the relation between the source and the target themselves, which is often a similarity."
            ],
            "category": "word analogy"
        },
        {
            "num": 2,
            "questions": [
                "Lexical inference",
                "What is lexical inference?"
            ],
            "responses": [
                "The meaning of an unknown word follows from the linguistic cues available in the text together with the reader’s knowledge.",
                "Lexical inference is a semantic relation from x -> y. A lexical inference is a direct semantic relation. Examples of such lexical inferences and their relations are synonyms, hyponyms. "
            ],
            "category": "lexical inference"
        },
        {
            "num": 2,
            "questions": [
                "What is connotation"
            ],
            "responses": [
                "Connotation is the different meanings a word can take according to the transmitter's or receiver's viewpoint",
                "Some words are related to emotions, sentiment, opinions or evaluations. These can be positive or negative and are called connotations. Words like happy have a very positive connotation, while crying has more of a negative connotation."
            ],
            "category": "connotation"
        },
        {
            "num": 2,
            "questions": [
                "What is td-idf?",
                "What is TD-IDF method"
            ],
            "responses": [
                "It stands for term frequency and inverse document frequency. It is a method to way co-occurance matrices. In this the value of the matrice increases if the word ccurs often in the text. But on the other hand the value is pulled back when it is an overall much ocurring word like the. Words that occur many times in a text but are not generic can say a lot about the text.",
                "Td-idf is a model in which nearby words that have a function of counts, are what defines a word.  This model is based on two terms: frequency and the inverse document frequency. The term frequency is basically the frequency of a word, while the inverse document frequency is used for the more discriminative words and appoints higher weights to them. This model is used to analyse similarity of two documents and analyse word similarity."
            ],
            "category": "td-idf"
        },
        {
            "num": 2,
            "questions": [
                "Conceptualization",
                "What is a conceptualization?"
            ],
            "responses": [
                "The abstract translation of the world grounded in action and perception systems of our body. The meaning of a word is a particular conceptualization of an entity or situation.",
                "This are mental experience of semantics and why some words are more popular in cognitive linguistics. It’s about new and existing ideas on semantics, abstract and emotional experiences and the understanding of social aspect for linguistic context. It is about giving meaning and explaining linguistic distributions. "
            ],
            "category": "conceptualization"
        },
        {
            "num": 2,
            "questions": [
                "Contextual representation",
                "What is a contextual representation?"
            ],
            "responses": [
                "An abstract cognitive representation of a word formed by repeated encounters of a word in a specific context",
                "Contextual representations are the different contexts in which a word can appear. The word 'bank' can for example appear in both 'I'm going to deposit my money at the bank' and 'I saw a duck at the riverbank'."
            ],
            "category": "contextual representation"
        },
        {
            "num": 2,
            "questions": [
                "Concreteness",
                "What is concreteness?"
            ],
            "responses": [
                "The extent to which a concept has a directly perceptible physical referent.",
                "Concreteness in linguistics is the amount of given information that is just enough to understand what's being said. Concreteness is one of Grice(1975)'s conversational maxims."
            ],
            "category": "concreteness"
        },
        {
            "num": 2,
            "questions": [
                "What is distributional hypothesis",
                "What is the distributional hypothesis"
            ],
            "responses": [
                "The distributional hypothesis states that words that occur in similar contexts have similar meanings",
                "Distributional hypothesis is the hypothesis that words with similar meanings tend to appear in similar contexts and the difference in meaning between the two words is roughly equal with the difference between the two contexts."
            ],
            "category": "distributional-hypothesis"
        },
        {
            "num": 2,
            "questions": [
                "What is a co-occurrence matrix",
                "What is a co-occurrence matrix?"
            ],
            "responses": [
                "Co-occurrence matrix is a vector space representation of co-occurrence(similarity) of words.",
                "A co-occurrence matrix is a scheme in which is noted how often two words occur in the context. The matrix is usually of size |V|x|V|, but for smaller data a window of +- 4 is used."
            ],
            "category": "co-occurrence matrix"
        },
        {
            "num": 2,
            "questions": [
                "What is the DH",
                "What is Distributional Hypothesis",
                "What is the Distributional Hypothesis"
            ],
            "responses": [
                "The Distributional Hypothesis states that the semantic similarity between two words depends on the similarity of the linguistic contexts in which they can occur",
                "The degree of semantic similarity between two linguistic expressions A and B is a function of the similarity of the linguistic contexts in which A and B can appear. Therefor, at least certain aspects of the meaning of lexical expressions depend on the distributional properties of such expressions, i.e. on the linguistic contexts in which they are observed."
            ],
            "category": "dh"
        },
        {
            "num": 2,
            "questions": [
                "What are gold labels"
            ],
            "responses": [
                "Gold labels are human annotations done to a text",
                "Gold labels are text classifiers imported by humans"
            ],
            "category": "gold labels"
        },
        {
            "num": 2,
            "questions": [
                "What is a Classifier"
            ],
            "responses": [
                "A Classifier is an algorithm that implements classification based on the features of the input given by a feature extractor",
                "A classifier is a model which aims at classifying the text or document into an existing class by taking the useful features of a given observation."
            ],
            "category": "classifier"
        },
        {
            "num": 2,
            "questions": [
                "What is supervised learning",
                "What is supervised learning?"
            ],
            "responses": [
                "Machine learning that uses labeled training data. The machine learning algorithm tries to find out which are the charachtericstics that make an element fit its label.",
                "Supervised learning refers to enabling computers to learn the relationship between an input and an output, through being fed a training data with many input observations with corresponding correct outputs. Once the relationship is learned, the computer is able to generalize it to predict the output, given a new observation."
            ],
            "category": "supervised learning"
        },
        {
            "num": 2,
            "questions": [
                "What is an F-measure",
                "classifier evaluation"
            ],
            "responses": [
                "The two most important metrics for evaluating the performance of a classifier are 'precision' and 'recall'. Precision is the proportion of items correctly returned for the right class label out of all the items returned, and recall is the proportion of returned items that were correctly classified as class c out of all items that are actually present in the system for c. Precision then filters false positives ('errors'), while recall serves as a filter for false negatives ('omissions'). F-measure is a single metric that combines both precision and recall: it is the harmonic mean of the two with weights that can be set for different applications.",
                "F-measure (also known as F1) is a way of eveluating the performance of a program. It is based computed using the precision and recall of the program. It is best explained by using a binary classification problem, like spam detection. The precision is the number of items the program classied correctly a spam divided by all items that the programm classied as spam, even if that was incorrect. The recall is the number of items that are classified correctly as spam divided by the number of items that should have been classified as spam. This a better measure than computing the accuracy (percentage of items correctly specified as spam) because it accounts for items that are falsy classified as spam. If this did not matter, a program could just classify everything as spam and then it would of course find all the spam emails, but it would not be a good classifier."
            ],
            "category": "f-measure"
        },
        {
            "num": 2,
            "questions": [
                " What is binary multinomial naive Bayes?",
                "What is binary multinomial naive Bayes (binary NB)?"
            ],
            "responses": [
                "Binary NB is a variant of sentiment analysis in which the frequency of a word seems less important than whether the word appears or not. A difference in this variant is that duplicate words are removed before they are linked together into a document.",
                "Binary NB is an analysis method used for sentiment measuring in text classification. It is dependent on word occurrence instead of word frequency. All word frequency measures are replaced by a value from the binary system, where 0 matches no occurrence, 1 one occurrence and 2 multiple occurrences."
            ],
            "category": "binary multinomial naive bayes"
        },
        {
            "num": 2,
            "questions": [
                "What is a bag-of-words",
                "What is a bag of words",
                "What is the bag-of-words assumption"
            ],
            "responses": [
                "In a bag-of-words, the words from a text are represented as an unordered set, where word position is not included as information, but word frequency is",
                "The bag-of-words model is a way to represent a text as a vector, where only the frequency of the word is stored. This means that word-order is not stored. "
            ],
            "category": "bag-of-words"
        },
        {
            "num": 2,
            "questions": [
                "What is Naive Bayes?",
                "What is machine learning?",
                "What model is Naive Bayes?"
            ],
            "responses": [
                "The goal of machine learning is to enable machines to learn from input by analyzing patterns, building models and calculate probabilities. It differs from rule-based learning by the fact that it is not pre-programmed with rules created by humans needed to analyze the data.",
                " Naive Bayes is a generative model for sentiment classification that makes use of the bag-of-words concept - the position of the words in a text does not matter. It is a generative model because it generates the probability that a certain class couls have generated the input document."
            ],
            "category": ""
        },
        {
            "num": 1,
            "questions": [
                "What is Turing's test?",
                "What does Turing's test test?"
            ],
            "responses": [
                "A test suggested by Alan Turing in 1950, where a machine and a human are sutiated in one room and are interrogated by a human in another room, whose goal is to figure out who is the machine. The machine tries to fool the interrogator and if succeeds it is deemed to be able to replicate the results of thinking. The tests only aims to estimate whether a machine can behave human rahter than have actual thinking processes."
            ],
            "category": "ai test"
        },
        {
            "num": 1,
            "questions": [
                "What is the Chinese room experiment?"
            ],
            "responses": [
                "The Chinese room experiment, conceived by John Searle, aims to show that reading and handling signs does not equal understanding their meanings, therefore a programmed computer merely simulates thinking rather than actually doing it. It suggests we imagine a man who does not speak Chinese sitting in a room, being handed written Chinese signs forming a question through one window and having to construct an answer in Chinese and handed it out through another window, solely depending on a book of rules which says what signs are approriate answer to the given ones. At no point does he learn the meaning of the signs."
            ],
            "category": "thought experiment"
        },
        {
            "num": 1,
            "questions": [
                "Is there a continuum between unembodiment and strong embodiment semantic theories?"
            ],
            "responses": [
                "Meteyard and Cuardado proposed a continuum between organizational semantic theories in 2012. On one end of the continuum there is the unembodied theory, where there is only arbitrary connection between form and meaning and no overlap between semantic and sensory-motor areas. On the other end there is the strong embodiment theory where the sensory-motor areas are activated during semantic processing. There are two more theories in the middle. The theories varry not only on level of embodiment but also neural architecture, explanation of interactions and so on."
            ],
            "category": "embodiment theories"
        },
        {
            "num": 1,
            "questions": [
                "What are the typical steps when processing text?"
            ],
            "responses": [
                "Processing text is separated in smaller tasks such as tokeization and sentence splitting, part-of-speach tagging, semantic role labeling, sentiment analysis, named entity recongition and so on."
            ],
            "category": "nlp-pipeline"
        },
        {
            "num": 1,
            "questions": [
                "What is a conversational agent"
            ],
            "responses": [
                "A conversational agent or dialog system is a computer system designed to understand natural language and respond using natural language."
            ],
            "category": "conversational-agent"
        },
        {
            "num": 1,
            "questions": [
                "What is the Chinese Room argument"
            ],
            "responses": [
                "The Chinese Room argument is an argument maintains that human-like behavior from a computer does not entail a human-like understanding of the topic at hand. Suppose someone with no knowledge of Chinese is in a closed room where they receive questions in Chinese as input, and – using a phrasebook with all possible questions and corresponding answers – gives a valid answer in Chinese as output. As a result, native speakers of Chinese might be fooled into thinking that this system understands Chinese. Likewise, other humans might be fooled into assuming a human-like understanding of a topic from a computer that simply mimics human behavior without any underlying comprehension."
            ],
            "category": "chinese-room-argument"
        },
        {
            "num": 1,
            "questions": [
                "What is strong AI vs. weak AI"
            ],
            "responses": [
                " Whereas strong AI asserts that machines that act intelligently have minds in the exact same way that humans do, weak AI, however, states that such machines merely model the mind humans have."
            ],
            "category": "strong-ai-vs-weak-ai"
        },
        {
            "num": 1,
            "questions": [
                "What is syntactic"
            ],
            "responses": [
                "Symbol manipulation based on a set of rules"
            ],
            "category": "syntactic"
        },
        {
            "num": 1,
            "questions": [
                "What is (Cognitive) embodiment"
            ],
            "responses": [
                "The degree of how much perception of words/symbols in the form of auditory, vision and motor information attached to these perception influences the meanings that are attached to words/symbols."
            ],
            "category": "(cognitive) embodiment"
        },
        {
            "num": 1,
            "questions": [
                "What is linguistic knowledge"
            ],
            "responses": [
                "The study of linguistic sounds (phonetics and phonology), meaningful components of words (morphology), structural relationships between words (syntax), meaning (semantics), study how language is used to accomplish goals (pragmatics) and linguistic units larger than a single utterance (discourse)."
            ],
            "category": "knowledge of language (linguistic knowledge)"
        },
        {
            "num": 1,
            "questions": [
                "What is hybrid intelligence"
            ],
            "responses": [
                "Hybrid intelligence is another approach to artificial intelligence. There are certain tasks which humans cannot compete with a computer. However, there are many more tasks that AI can't even get closer to human intelligence. Hybrid intelligence is expanding human intelligence instead of replacing it."
            ],
            "category": "hybrid intelligence"
        },
        {
            "num": 1,
            "questions": [
                "What is word ambiguity"
            ],
            "responses": [
                "A word or a sentence might have several meanings within different contexts. Ambiguity is when the meaning of a word, phrase, or sentence is uncertain. Therefore, it is difficult to limit the definition of the words. Different meanings create plenty of probabilities. Additionally, words gain new meanings in context by the time."
            ],
            "category": "word ambiguity"
        },
        {
            "num": 1,
            "questions": [
                "What is a chatbot"
            ],
            "responses": [
                "A chatbot is a computer program that simulates human conversation through voice commands or text chats or both. Chatbot is an Artificial Intelligence feature that can be used through any major messaging applications but it is pretty limited because there is very little awareness of the full discourse. "
            ],
            "category": "chatbots"
        },
        {
            "num": 1,
            "questions": [
                "What are machine learning models?"
            ],
            "responses": [
                "These are systems used on large corpora, to retrieve some causal models from. Machine learning is used as a combination of automata, rule systems, search heuristics and classifiers. This combination of automata (finite machines as a way to characterize grammar) , heuristics ( structures of texts as a tool for semantics, pragmatics and discourse), rule systems (a tool for understanding phonology, morphology and syntax) can be used as a model to resolve linguistic ambiguities. "
            ],
            "category": "machine learning models"
        },
        {
            "num": 1,
            "questions": [
                "What are the four paradigms?"
            ],
            "responses": [
                "Those are based on language and speech processing. Stochastic: looks into the recognition of speech using algorithms. Logic based: looks at how language is structured. Natural language understanding: based on understanding of human concepts. Things that go beyond language, namely plans and goals. Discourse modelling: a paradigm that developed logic based speech. ."
            ],
            "category": "four paradigms"
        },
        {
            "num": 1,
            "questions": [
                "What are embodied approaches?"
            ],
            "responses": [
                "It is an approach that focusses on the idea that external world and cognition is about real world action instead of symbolic representation. There is controversy with the symbolic view. According to the embodied approach the world is created inside the brain by the outside world: sensory and motor information. Different amounts of sensory and motor information are used, since it depends on the difficulty of semantic processing. Therefore, it plays an important role in semantic representations (abstract language which contains meaning: resolving ambiguity, sentence comprehension). Meaning that different levels of embodiment helps to classify the meaning of the semantic representations. Unembodied theory in this sense means there is no motor and sensory information. Information provided is only symbolic. Which means there is no need to resolve ambiguity. Secondary embodiment meaning there might be activation of the sensory and motor information, but not all the time. Only when there is a specific concept presented that is ambiguous for example, there might be a passive activation of motor and sensory. Weak embodiment: meaning that sensory and motor information have at least a small effect on semantic representations. Strong embodiment: there is a routine of semantic processing. Therefor there is a low level of sensory and motor information. "
            ],
            "category": "embodied approaches"
        },
        {
            "num": 1,
            "questions": [
                "Turing's test",
                "What is imitation game"
            ],
            "responses": [
                "Imitation game is a game proposed by Alan Turing to help answer the question whether a computer can think. In short, if the computer can play the game as good as a person can that would be a significant milestone and good indication of computer intelligence"
            ],
            "category": "imitation game"
        },
        {
            "num": 1,
            "questions": [
                "Chinese room argument",
                "What is Chinese room agrument"
            ],
            "responses": [
                "Chinese room argument is Searle's argument againts machine intelligence with which he says that machines can only follow commands, and they can never really understand anything, let alone intelligently think. The argument claims that successfully manipulating Chinese symbols does not mean that one who manipulates them really understands them"
            ],
            "category": "chinese room argument"
        },
        {
            "num": 1,
            "questions": [
                "ELIZA",
                "What is ELIZA"
            ],
            "responses": [
                "ELIZA is an early language processing program developed by Wizenbaum, capable of holding short conversations with the user based on pattern-matching techniques"
            ],
            "category": "eliza"
        },
        {
            "num": 1,
            "questions": [
                "what is ambiguity"
            ],
            "responses": [
                "Ambiguity is the phenomenon of multiple alternative linguistic structures that can be built for some input."
            ],
            "category": "ambiquity"
        },
        {
            "num": 1,
            "questions": [
                "what is a communicative agent"
            ],
            "responses": [
                "A communicative agent is a machine capable of conversational communication through natural speech and language processing."
            ],
            "category": "communicative agent"
        },
        {
            "num": 1,
            "questions": [
                "What is the Cognitive Embodiment hypothesis?"
            ],
            "responses": [
                "IThe Cognitive Embodiment hypothesis states that semantic representation or processing can take place due to sensory and motor information in the observable environment of the perceiver. The extent to which semantic processing relies on this kind of information varies for different theories, but it is very unlikely that it plays no role at all."
            ],
            "category": "cognitive embodiment hypothesis"
        },
        {
            "num": 1,
            "questions": [
                "What is the difference between strong and weak AI?"
            ],
            "responses": [
                " Strong AI refers to a type of AI that can perform relatively simple tasks but does not show any signs of thinking in the way that humans do. Weak AI tend to simulate human behavior rather than replicate cognition. Strong AI refers to the (theoretical) type of AI that possesses cognition that is similar to human cognition in the sense that it can think, understand, process their environment in an independent manner that does not simply follow a step-by-step program."
            ],
            "category": "strong/weak ai"
        },
        {
            "num": 1,
            "questions": [
                "What is information extraction"
            ],
            "responses": [
                "Information extraction is finding and extracting relevant information on a specific topic from large amounts of texts with the help of machines/programming"
            ],
            "category": "information-extraction"
        },
        {
            "num": 1,
            "questions": [
                "What is machine learning"
            ],
            "responses": [
                "Machine learning is inputting large amounts of data into an algorithm and 'training' it to find patterns and probabilities in order to create more realistic outcomes. The goal is for a machine to learn and improve itself by evaluating input/output results"
            ],
            "category": "machine-learning"
        },
        {
            "num": 1,
            "questions": [
                "What is probabilistic parsing"
            ],
            "responses": [
                "Probabilistic parsing is parsing a sentence by statistically computing the most likely outcome based on a language's syntactic structure"
            ],
            "category": "probabilistic-parsing"
        },
        {
            "num": 1,
            "questions": [
                "What is word sense disambiguation"
            ],
            "responses": [
                "Word sense disambiguation is determining the sense or meaning of a word in a specific context. In the context of NLP, disambiguation of words in context is still a work in progress. Various methods can be used, from dictionary entries to machine learning"
            ],
            "category": "word-sense-disambiguation"
        },
        {
            "num": 1,
            "questions": [
                "What is weak embodiment",
                "What is strong embodiment",
                "What is the difference between weak and strong embodiment"
            ],
            "responses": [
                "Weak embodiment states that semantic processing involves some sensory and motor information, but that there is still some independent abstract representation of semantics, which is used for 'routine' semantic processing. According to strong embodiment, all semantic processing is fully dependent on sensory and motor information"
            ],
            "category": "weak-strong-embodiment"
        },
        {
            "num": 1,
            "questions": [
                "What are convergence zones"
            ],
            "responses": [
                "Convergence zones are regions of the cortex that store information that should be sufficient for semantic representation, including statistical regularties, feature conjuctions and correlations"
            ],
            "category": "convergence-zones"
        },
        {
            "num": 1,
            "questions": [
                "what is Natural language processing (NLP)"
            ],
            "responses": [
                "Natural language processing (NLP) is a multidisciplinary field that aims to make computers generate reasonable and understandable output by analyzing data from natural human languages."
            ],
            "category": "natural language processing (nlp)"
        },
        {
            "num": 1,
            "questions": [
                "embodied cognition"
            ],
            "responses": [
                "There are various semantic theories that maintain that meaning representations in the brain are in one way or another grounded in sensory and motor information. Whether this is the case has important consequences for AI. If meaning representations really are embodied to a certain extent, it might be impossible to build a machine with human intellect solely based on a computer program."
            ],
            "category": "embodied cognition"
        },
        {
            "num": 1,
            "questions": [
                "symbolic cognition"
            ],
            "responses": [
                "According to some semantic theories meaning representations are completely symbolic without any sensory and motor information grounding them. As an example, one of these theories, Latent Semantic Analysis (Landauer & Dumais, 1997), claims meaning is represented by a network of words where related words are more connected to each other. This idea is remarkably similar to how WordNets are organized."
            ],
            "category": "symbolic (unembodied) cognition"
        },
        {
            "num": 1,
            "questions": [
                "What are conversational agents?"
            ],
            "responses": [
                "The long-term focus in the AI-field: artificial entities which communicate conversationally."
            ],
            "category": "conversational agents"
        },
        {
            "num": 1,
            "questions": [
                "What is the embodiment hypothesis"
            ],
            "responses": [
                "The embodiment hyptothesis is the theory that sensory and motor information is necessary for semantic representation and processing"
            ],
            "category": "embodiment-hypothesis"
        },
        {
            "num": 1,
            "questions": [
                "What is the imitation game"
            ],
            "responses": [
                "The imitation game is a test for functional equivalence, later known as the Turing Test"
            ],
            "category": "imitation-game"
        },
        {
            "num": 1,
            "questions": [
                "What is Weak AI"
            ],
            "responses": [
                "Weak AI is an approach to AI in which computer programs simulating human cognition are only useful to study the human mind and nothing more"
            ],
            "category": "weak-ai"
        },
        {
            "num": 1,
            "questions": [
                "What is simulation?"
            ],
            "responses": [
                "Simulation is the imitation of a process."
            ],
            "category": "simulation"
        },
        {
            "num": 1,
            "questions": [
                "What is duplication?"
            ],
            "responses": [
                "Duplication is the perfect reproduction of a process"
            ],
            "category": "duplication"
        },
        {
            "num": 1,
            "questions": [
                "What is cognitive embodiment?"
            ],
            "responses": [
                "Cognitive embodiment is the idea that physical properties of the body have a causal or constitutive function in cognitive processes. According to this theory, semantic representations are essentially sensory and motor information."
            ],
            "category": "cognitive embodiment"
        },
        {
            "num": 1,
            "questions": [
                "What is natural language understanding?"
            ],
            "responses": [
                "One of the hard-AI problems, it refers to computer programs that allow machines to take in text or speech as input and output the adequate response, enabling human-computer interactions through language."
            ],
            "category": "natural language understanding"
        },
        {
            "num": 1,
            "questions": [
                "What are speech and language processing systems?"
            ],
            "responses": [
                "They are a​pplication ​softwares that processe spoken and written language by employing knowledge of the formal aspects of each language level (e.g. phonology, syntax, pragmatics...)."
            ],
            "category": "speech and language processing systems"
        },
        {
            "num": 1,
            "questions": [
                "What is Formal Language Technology?"
            ],
            "responses": [
                "Formal language technology was a new technology in the field of linguistics that arose around the 1950 and 1960’s. One of the biggest names for this technology is Chomsky, who designed one of the first formal grammars for natural languages. With the introduction of formal grammar, there came more in-depth possibilities for linguistic research in combination with AI technologies."
            ],
            "category": "formal language technology"
        },
        {
            "num": 1,
            "questions": [
                "What is The Embodyment Hypothesis?"
            ],
            "responses": [
                "The embodiment hypothesis is based on the connection between sensory an motor information and semantic representation. It explores to which extend motor and sensory information is necessary and relevant for semantic representation. Semantic representation is the meaning we assign to a lexical item in our mental lexicon and motor and sensory information is the information from things we encounter in life that we might link to those lexical items."
            ],
            "category": "the embodiment hypothesis"
        },
        {
            "num": 1,
            "questions": [
                "What is Layered processing:"
            ],
            "responses": [
                "Forming a number of layers of neural network, to observe and validate."
            ],
            "category": "layered processing:"
        },
        {
            "num": 1,
            "questions": [
                "What is NLP"
            ],
            "responses": [
                "Make the machine understand the language the human use."
            ],
            "category": "nlp"
        },
        {
            "num": 1,
            "questions": [
                "What is Indeterminacy of reference"
            ],
            "responses": [
                "Human refer to some object or solve problems building on top of previous knowledge or experience they have been through."
            ],
            "category": "indeterminacy of reference"
        },
        {
            "num": 1,
            "questions": [
                "What is AI"
            ],
            "responses": [
                "Intelligence made by human make the machine take decisions just like humans."
            ],
            "category": "ai"
        },
        {
            "num": 1,
            "questions": [
                "What are models"
            ],
            "responses": [
                "Models are representations of information processing systems, for example for processing language, such as finite-state-machines."
            ],
            "category": "models"
        },
        {
            "num": 1,
            "questions": [
                "What is thinking"
            ],
            "responses": [
                "What is thinking is kind of the big question: what is thinking? Can it be the same for humans and machines? For humans it could be: internal thought processes used for (but not limited to) performing tasks.For machines it could be: the process of searching for and finding solutions to problems, making the machine capable of performing tasks."
            ],
            "category": "thinking"
        },
        {
            "num": 1,
            "questions": [
                "What is a machine"
            ],
            "responses": [
                "A machine is a  physical system capable of performing certain functions."
            ],
            "category": "machine"
        },
        {
            "num": 1,
            "questions": [
                "What is intelligence"
            ],
            "responses": [
                "Intelligence is the capability of thinking?"
            ],
            "category": "intelligence"
        },
        {
            "num": 1,
            "questions": [
                "What is a computer program"
            ],
            "responses": [
                "A computer program is a set of commands and rules to instruct a machine to perform a certain function."
            ],
            "category": "computer program"
        },
        {
            "num": 1,
            "questions": [
                "What is the chinese room"
            ],
            "responses": [
                "An argument against the Turing Test, saying that a person looking at a rule book and coming up with the proper response intuitively does not have intelligence as it does not understand the questions."
            ],
            "category": "chinese room"
        },
        {
            "num": 1,
            "questions": [
                "What is are embodiment architectures"
            ],
            "responses": [
                "a continuum of which one end evolves around no embodiment (mostly symbols) and the other end evolves around full embodiment including auditory, visionary and motoric sensors"
            ],
            "category": "embodiment architectures"
        },
        {
            "num": 1,
            "questions": [
                "What is layered processing"
            ],
            "responses": [
                "a way of information in which information is learned sequentially, by learning it piece by piece (in each layer)."
            ],
            "category": "layered processing"
        },
        {
            "num": 1,
            "questions": [
                "What is NLP Pipeline"
            ],
            "responses": [
                "A hierarchical structure in which natural language processing tasks, of which each can be solved with different techniques, are ordered based on their dependencies on each other."
            ],
            "category": "nlp pipeline"
        },
        {
            "num": 1,
            "questions": [
                "What is Speech and Language Processing: "
            ],
            "responses": [
                "The technology/theories behind computers / machines that have natural language as input, process that input and give a natural language output. Just like humans do. In order to this the computer has to have extensive knowledge about the following six categories: phonetics & phonology; morphology; syntax; semantics; pragmatics; discourse."
            ],
            "category": "speech and language processing:"
        },
        {
            "num": 1,
            "questions": [
                "What is the Turing Test?"
            ],
            "responses": [
                "A hypothetical game created by Alan Turing. A human has to speak with a computer/machine and another human for a set period of time. Both the human and the machine try to convince the interrogator that he/it is the human. If the machine manages to fool the interrogator he has won the game. In such way we can test not if a computer can think but rather if it can act undistinguishably from someone that is a thinker."
            ],
            "category": "turing test:"
        },
        {
            "num": 1,
            "questions": [
                "What is embodied semantics?"
            ],
            "responses": [
                "There is some sort of relationship between the format in which semantic information is represented when compared to the information to which it refers. In other words the semantic information depends to some extent to the sensory and motor systems of the brain. There are different kind of categories within this theory: symbolic (no relationship whatsoever – not supported ), secondary (only acknowledge a non-arbitrary relationship), weak (partly dependent on sensory and motor systems), strong (completely dependent – not supported)."
            ],
            "category": "embodied semantics:"
        },
        {
            "num": 1,
            "questions": [
                "What are convergence zones?"
            ],
            "responses": [
                "For semantic representations many theories rely on the assumption that in order to so we need to collect information that stands for the thing. Convergence zones are areas in the brain that can store this information. All theories of embodiment rely in different ways on these convergence zones."
            ],
            "category": "convergence zones"
        },
        {
            "num": 1,
            "questions": [
                "What is NLG",
                "What is Natural Language Generation (NLG)"
            ],
            "responses": [
                "Natural Language Generation (NLG) is software such as NLTK or SpaCy can transform structural data to natural language"
            ],
            "category": "natural language generation (nlg)"
        },
        {
            "num": 1,
            "questions": [
                "What is Speech Synthesis"
            ],
            "responses": [
                "Speech Synthesis is in short, the way computers produce human speech. In other words, we could say that a computer with a “voice” is a speech synthesizer, e.g. Text-aid for students with dyslexia"
            ],
            "category": "speech synthesis"
        },
        {
            "num": 1,
            "questions": [
                "What is Probabilistic Parsing"
            ],
            "responses": [
                "Probabilistic Parsing is using dynamic programming algorithms to compute the most likely parse(s) of any given sentence"
            ],
            "category": "probabilistic parsing"
        },
        {
            "num": 1,
            "questions": [
                "What is WSD",
                "What is Word Sense Disambiguation (WSD)"
            ],
            "responses": [
                "Word Sense Disambiguation (WSD) is an open problem concerned with identifying which sense of a word is used in a sentence and PYWSD is a simple library that wrap two WSD methods or tools, i.e. NLTK and Babelfy which implements word sense disambiguation technology. This technology can solve the problem by determining which “sense” of meaning of a particular word is activated by the use a word in that specific context."
            ],
            "category": "word sense disambiguation (wsd)"
        },
        {
            "num": 1,
            "questions": [
                "What is Unitary Semantic System"
            ],
            "responses": [
                "Unitary Semantic System is acquired through the same set of modalities of input and output which run through the relation between different kinds of sensory and motor information sets"
            ],
            "category": "unitary semantic system"
        },
        {
            "num": 1,
            "questions": [
                "Synonym sets"
            ],
            "responses": [
                "In WordNet, nouns are assigned to synonym sets (synsets) based on the lexical relation of synonymy. Nouns in a given synset express the same concept and it is on the basis of the meaning relations of these synsets the network of nouns is organized in WordNet. For instance, a potential synset in WordNet is synset_carpet_1 (‘a fuzzy item under our feet in a room’) whose membership contains the words carpet and rug. This synset then might be a hyponym of synset_furniture_1 which in turn is a hyponym of synset_objects_1, etc."
            ],
            "category": "synonym sets"
        },
        {
            "num": 1,
            "questions": [
                "What is The Lexical Matrix"
            ],
            "responses": [
                "Is part of our language as human and it shows the difference of a word between its form and meaning."
            ],
            "category": "the lexical matrix"
        },
        {
            "num": 1,
            "questions": [
                "What is Information Content-based Measure"
            ],
            "responses": [
                "Every concept holds more information than what it seems. Measuring how close the two concepts to each other, we can specify how similar they are."
            ],
            "category": "information content-based measure"
        },
        {
            "num": 1,
            "questions": [
                "What is Path-based Measures"
            ],
            "responses": [
                "The similarity between two concepts rely on the length of the path which link the two concepts."
            ],
            "category": "path-based measures"
        },
        {
            "num": 1,
            "questions": [
                "WHat is a lexical matrix"
            ],
            "responses": [
                "The schematic representation of the mapping between the word form and the word meaning. Also, in WordNet, the mapping between synsets and written words."
            ],
            "category": "lexical matrix:"
        },
        {
            "num": 1,
            "questions": [
                "What are semantic similarity measures:"
            ],
            "responses": [
                "Semantic similarity measures: Used to determine the semantic relations used in WordNet 1. Path based: The similarity between two concepts is a function of the length of the path linking the concepts and the position of the concepts in the taxonomy. 2. IC based: The more common information two concepts share, the more similar the concepts are. 3. Feature based: Concepts with more common features and less non-common features are more similar. 4. Combinations"
            ],
            "category": "semantic similarity measures:"
        },
        {
            "num": 1,
            "questions": [
                "What is a synonym"
            ],
            "responses": [
                "Of course most people are familiar with the term synonym, but a formal definition is a bit more difficult to give. Leibniz said that if two expressions are synonymous, then the substitution of one for the other never changes the truth value of a sentence in which the substitution is made. This, however, is a rather strong definition, which excludes a lot of word pairs that are considered synonyms. A weaker definition seems to make more sense: two expressions are synonymous in a linguistic context C if the substitution of one for the other in C does not alter the truth value."
            ],
            "category": "synonym"
        },
        {
            "num": 1,
            "questions": [
                "What is san antonym"
            ],
            "responses": [
                "Often the antonym of word x is explained as not-x. This is not quite precise enough as the negation of a term is the same as the opposite. A synonym x^'of x does not have the same antonym as x, even though their word meanings should be roughly the same."
            ],
            "category": "antonym"
        },
        {
            "num": 1,
            "questions": [
                "What is a hyponym",
                "What is a hypernym"
            ],
            "responses": [
                "The hyponym can be described as a ISA relation. To make this more clear, the sentence An x is a (kind of) y should be true for a certain x and y. Here x is a subset of y, and y a superset of x. The subset always inherits all the features of the superset and then has some additional ones that make it different from the superset."
            ],
            "category": "hyponym"
        },
        {
            "num": 1,
            "questions": [
                "What is a meronym"
            ],
            "responses": [
                "A meronym is a less familiar kind of relation than the aforementioned. This relation can be described with a similar kind of sentence as with the hyponym: A x is a part of y. This can also be described as the HASA relation."
            ],
            "category": "meronym"
        },
        {
            "num": 1,
            "questions": [
                "What is a Is A-Relation?"
            ],
            "responses": [
                "Is A-relation, also known as hyponym/hypernym relation, is the most common relation between nouns. A hyponym is a type of hypernym. An example of this is: a robin is a type of bird. A bird then again is a type of animal, and an animal is a type of creature. The lower you get in hierarchy of the hyponyms, the more concrete the concepts get. A hyponym takes all the features of the hypernym and adds more features to it, to distinguish it from other concepts on the same level below the hypernym."
            ],
            "category": "is a-relation"
        },
        {
            "num": 1,
            "questions": [
                "What is Psych lexicology"
            ],
            "responses": [
                " Those are studies about lexical components. As a start of the wordnets, the first thing which was important was lexical knowledge. Researchers found that lexicons should exist of phonetics, syntactic and lexical components so it would be understandable for studying linguistics. As work proceeded wordnets arose.  "
            ],
            "category": "psych lexicology"
        },
        {
            "num": 1,
            "questions": [
                "What are wordnets?"
            ],
            "responses": [
                "Are systems designed by psycholinguistic theories of human language memories. It is a large lexical database. Wordnets are lexical reference systems and organize nouns, verbs and adjectives into synonym sets which represent lexical concepts. Wordnets contain different word forms, word meanings, sets of synonyms. Synonymy (word form): it is the most important relation for wordnets and is used in all reference systems. Synonymy meaning that there is similarity of meaning. Antonymy (word form): it is something, but it does not have to mean another thing. As stated in the article of miller et al if you are not rich you don’t have to be poor immediately. It is used for adjectives and adverbs in wordnets.Hyponymy (word meaning): it is a concept that is expressed by the synset. Similarity between concepts. It is used for the nouns in wordnets. Meronymy: it is a problem to organize lexical knowledge associated with different syntactic categories. Morphological Relations: relation between word forms. "
            ],
            "category": "wordnets"
        },
        {
            "num": 1,
            "questions": [
                "What are lexical semantics"
            ],
            "responses": [
                " it is an idea that lexicalized concept and expressions are associated. Both are important in syntactic roles. Lexalized concept consist of word meaning and expressions consist of word forms. "
            ],
            "category": "lexical semantics"
        },
        {
            "num": 1,
            "questions": [
                "What are semantic similarity and relatedness"
            ],
            "responses": [
                "Semantic similarity is about the idea of distance between items is based on the likeness of their meaning or semantic content as opposed to lexicographical similarity. These are mathematical tools used to estimate the strength of the semantic relationship between units of language, concepts or instances, through a numerical description obtained according to the comparison of information supporting their meaning or describing their nature. Semantic relatedness includes any relation between two terms, while semantic similarity only includes relations. For example, 'pan' is similar to 'pot', but is also related to 'kitchen' and 'cooking'."
            ],
            "category": "semantic similarity and relatedness"
        },
        {
            "num": 1,
            "questions": [
                "what is Mental lexicon"
            ],
            "responses": [
                "Mental lexicon is a human storage memory of lexicon which serves as the knowledge and recognition of words of language users."
            ],
            "category": "mental lexicon"
        },
        {
            "num": 1,
            "questions": [
                "What is psycholinguistics"
            ],
            "responses": [
                "Psycholinguistics is the field of research that focuses on the cognitive processes that influence linguistic capabilities."
            ],
            "category": "psycholinguistics"
        },
        {
            "num": 1,
            "questions": [
                "What is the iformation content"
            ],
            "responses": [
                "Information content is the information provided by a word."
            ],
            "category": "information content"
        },
        {
            "num": 1,
            "questions": [
                "What is word similarity?"
            ],
            "responses": [
                "Word similariy is how similar the meaning of two or more words is, probably measured in a scale. Synonyms would for example be (almost) 100% similar."
            ],
            "category": "word similarity"
        },
        {
            "num": 1,
            "questions": [
                "What is a hypernymy"
            ],
            "responses": [
                "hypernymy is a relation between words, where one word contains the semantic value of another word, but adds more specific information to it. The first word is then a hypernym of the second word."
            ],
            "category": "hypernymy"
        },
        {
            "num": 1,
            "questions": [
                "What is a semantic similarity measure"
            ],
            "responses": [
                "A semantic similarity measure is a way of measuring similarity between lexical concepts by using the length of a path between concepts, the amount of common information or features that two concepts share or a combination of these three"
            ],
            "category": "semantic-similarity- measure"
        },
        {
            "num": 1,
            "questions": [
                "meronymy",
                "synonymy",
                "Word relations"
            ],
            "responses": [
                "Meronymy describes a certain semantic relationship between two semantic items. The relationship that is meant by the term meronymy is that a given concept is part of another concept. A “finger” for example, is a meronymy for “hand” since a finger is a part of a hand. Synonymy described the phenomenon when different semantic entities have the same or very close related meaning. Hyponymy describes the phenomenon of a lexical entity being a type of some other lexical entity (its hypernym). For example the word yellow is a hyponym of the hypernym colours. "
            ],
            "category": "word relations"
        },
        {
            "num": 1,
            "questions": [
                "Similarity and relatedness"
            ],
            "responses": [
                "relatedness measures the degree to which two concepts are related to each other in terms of association. Similarity is a subset of of relatedness and  "
            ],
            "category": "similarity and relatedness "
        },
        {
            "num": 1,
            "questions": [
                "Zipf’s Law"
            ],
            "responses": [
                "This empirical law refers to the fact that most types of data are distributed in a comparable way, called an Zipfiean distribution. This term describes the phenomenon in statistics in which (in lexical data for instance) the highest word occurrence divided by 2 gives the number of second most occurences. Zipf’s law was formulated for the field of quantitative linguistics, but was discovered to be relatable for all sorts of data like the number of citizens for the top 10 biggest cities in the United States. "
            ],
            "category": "zipf’s law "
        },
        {
            "num": 1,
            "questions": [
                "What is stemmming"
            ],
            "responses": [
                "Stemming is the process of eliminating affixes (suffixed, prefixes, infixes, circumfixes) from a word in order to obtain a word in its root form; for instance, runs, running and ran are all from the same stem of run."
            ],
            "category": "stemming"
        },
        {
            "num": 1,
            "questions": [
                "What is lemmatization"
            ],
            "responses": [
                "Lemmatization is related to stemming, differing in that lemmatization is able to capture canonical forms based on a word's lemma (dictionary form). For example, stemming the word 'worse' would fail to return its citation form (another word for lemma); however, lemmatization would result in the following: “worse” → “bad”."
            ],
            "category": "lemmatization"
        },
        {
            "num": 1,
            "questions": [
                "What is a bag of words"
            ],
            "responses": [
                "The bag of words model omits grammar and word order but is interested in the number of occurrences of words within the text.  This is a representation used for the processing of text and does not include the word order but is useful for extracting the frequency of a word."
            ],
            "category": "bag of words"
        },
        {
            "num": 1,
            "questions": [
                "What is sematic analysis"
            ],
            "responses": [
                "Semantic analysis can be explained in contrast with syntactic analysis. The latter describes which words a text consists of and semantic analysis is concerned with what he collection of words actually means. Semantic analysis is carried out once the text has been read and parsed (analysed syntactically)."
            ],
            "category": "semantic analysis"
        },
        {
            "num": 1,
            "questions": [
                "How can we measure semantic similarity?",
                "What is path based semantic similarity measures?"
            ],
            "responses": [
                "A way of defining how close or not are two word meanings by measuring the path between their nodes in a taxonomy. There are different ways of calculating that, as some take into account the most specific common concept instead of the maximum depth of the taxonomy."
            ],
            "category": "path-measures"
        },
        {
            "num": 1,
            "questions": [
                "How can we measure semantic similarity?",
                "What is information content based similarity measures?"
            ],
            "responses": [
                "Another way of measuring how close two word meanings are, based on the information that subsumes them in the technology. It relies on statistical analysis of corpora or on the amount of hyponyms it has as registered in WordsNet."
            ],
            "category": "content-measures"
        },
        {
            "num": 1,
            "questions": [
                "How can we measure semantic similarity?",
                "What is ifeature based similarity measures?"
            ],
            "responses": [
                "Yet another way to measure semantic similarity, relying on the features that two word concepts share or not, found in their definitions or glosses in WordsNet."
            ],
            "category": "feature-measures"
        },
        {
            "num": 1,
            "questions": [
                "What is polysemy?"
            ],
            "responses": [
                "Polysemy is when a particular word has more meanings. For instance, the word bank. Bank can be an institution where your money is stored, but bank can also be a side of a river. "
            ],
            "category": "polysemous"
        },
        {
            "num": 1,
            "questions": [
                "what is antonomy"
            ],
            "responses": [
                "The antonym of a word x is (sometimes) not-x (but not always). "
            ],
            "category": "antonomy"
        },
        {
            "num": 1,
            "questions": [
                "what is hyponomy"
            ],
            "responses": [
                "A concept x is a hyponym of a concept y if an x is a (kind of) y."
            ],
            "category": "hyponomy"
        },
        {
            "num": 1,
            "questions": [
                "What is Word Net"
            ],
            "responses": [
                "Word Net is a large lexical database that takes not only word meaning and word form into consideration, also the semantic relations between words are given, where Hyponym/hypernym forms 80% of the most common relations"
            ],
            "category": "word net"
        },
        {
            "num": 1,
            "questions": [
                "What is Construction Theory"
            ],
            "responses": [
                "Construction Theory is when the representation of word meanings are explained in such a way that the given information is understood by people with limited knowledge of the language, e.g., English, i.e. not just a mere synonym, but rather support enough examples and information of the word meaning"
            ],
            "category": "construction theory"
        },
        {
            "num": 1,
            "questions": [
                "What is Differential theory"
            ],
            "responses": [
                "Differential theory is when word meanings are based on any symbol needed to show their similarities or differences"
            ],
            "category": "differential theory"
        },
        {
            "num": 1,
            "questions": [
                "What is Morphological Relation"
            ],
            "responses": [
                "Morphological Relation is a nifty element of Word Net, where the user can hover with their mouse over a word and Word Net returns the stem/base of a word or break it down in pre/suffixes (inflectional morphology). Word Net’s users are still waiting for the more advanced derivational morphological element"
            ],
            "category": "morphological relation"
        },
        {
            "num": 1,
            "questions": [
                "What is (word) Ambiguity and Variation"
            ],
            "responses": [
                "Respectively a word having more than one meaning and multiple words referring to the same thing in the real world. "
            ],
            "category": "(word) ambiguity and variation"
        },
        {
            "num": 1,
            "questions": [
                "What is a lexical database"
            ],
            "responses": [
                "models based on the hypothesis that context infer word meanings"
            ],
            "category": "lexical database"
        },
        {
            "num": 1,
            "questions": [
                "What is zipfian distribution"
            ],
            "responses": [
                "Frequency distribution of word occurrences found in textual resources."
            ],
            "category": "zipfian distribution"
        },
        {
            "num": 1,
            "questions": [
                "What are text corpora"
            ],
            "responses": [
                "Text samples that can be used for models to interpret the meanings of words in context"
            ],
            "category": "text corpora"
        },
        {
            "num": 1,
            "questions": [
                "What is (Word) Similarity"
            ],
            "responses": [
                "a concept describing how words look like each other in terms of meaning, being compared by word vectors and word representations."
            ],
            "category": "(word) similarity"
        },
        {
            "num": 1,
            "questions": [
                "What is psycholexicology"
            ],
            "responses": [
                "Psycholexicology is he combination of lexicology with psycholinguistics. In lexicology research and lexicography more and more psycholinguistic data is used in order to determine what should be included into a lexicon. A lot of data about a human's mental lexicon has been able to be incorporated into lexicography"
            ],
            "category": "psycholexicology"
        },
        {
            "num": 1,
            "questions": [
                "What is constructive and differential theory"
            ],
            "responses": [
                "Constructive and differential theory are two theories about how lexical concepts should be represented by definitions. Constructive theory suggests that a definition should contain enough information to be able to construct or represent the concept accurately. Differential theory on the other hand, suggests that a definition does not have to be exhaustive and any symbol that makes it possible to differentiate between the concepts is enough. WordNet takes the differential approach, using synonyms to relate between concepts"
            ],
            "category": "constructive-and-differential-theory"
        },
        {
            "num": 1,
            "questions": [
                "What is word semantic similarity"
            ],
            "responses": [
                "Semantic similarity is he relation between two words, or utterances, documents, etc., is measured in how similar two meanings are, instead of their form or their alphabetical order for example. On WordNet, semantic similarity is very important as it is used to map relations between words"
            ],
            "category": "semantic-similarity"
        },
        {
            "num": 1,
            "questions": [
                "What is a lexical relation"
            ],
            "responses": [
                "A lexical relation is a relation between word forms, such as synonymy and antonymy"
            ],
            "category": "lexical-relation"
        },
        {
            "num": 1,
            "questions": [
                "Distributional hypothesis",
                "What is the distribution hypothesis?"
            ],
            "responses": [
                "The distributional hypothesis states that semantic similarity can be defined in terms of linguistic distributions. This means that the meaning of a linguistic entity is defined by the surrounding linguistic entities. Therefore, by inspecting the semantic context of an entity, one should be able to derive information about semantic properties of this entity. As described by Lenci, 2008: 'The degree of semantic similarity between two linguistic expressions A and B is a function of the similarity of the linguistic contexts in which A and B can appear.' "
            ],
            "category": "distribution hypothesis"
        },
        {
            "num": 1,
            "questions": [
                "BoW",
                "Bag of Words",
                "Bag of Words Model"
            ],
            "responses": [
                "The Bag of Words (or BoW) model is a model that is used to provide an adequate yet simplified representation of the important items in a given text. This model is often used in Natural Language Processing to determine which entities of a text are valuable in data for learning algorithms. The BoW model uses the unique items in a corpus to turn into a vector that can be used as input for a machine learning model. For this model, n-grams of any number of tokens can be used.  "
            ],
            "category": "bag of words model "
        },
        {
            "num": 1,
            "questions": [
                "LSI",
                "LSA",
                "Latent Semantic Analysis"
            ],
            "responses": [
                "LSA also known as Latent Semantic Index us a technique for information retrieval in Natural Language Processing that is used to analyze the relationship between given documents by using the Distributional Hypothesis, the main idea of this technique is that words that are close in meaning will occur in similar parts of the document. LSA uses the BoW Model to construct a term-document matrix in which the occurrence of certain terms in a document are represented. The weight of an entity in the matrix is determined by the number of times this term appears in both documents. "
            ],
            "category": "latent semantic analysis "
        },
        {
            "num": 1,
            "questions": [
                "What is distributional model"
            ],
            "responses": [
                "models based on the hypothesis that context infer word meanings"
            ],
            "category": "distributional model"
        },
        {
            "num": 1,
            "questions": [
                "What is word embedding"
            ],
            "responses": [
                "Representation of a word as a vector based on the context the word is in."
            ],
            "category": "word embedding"
        },
        {
            "num": 1,
            "questions": [
                "Wat is the similarity/association distinction?"
            ],
            "responses": [
                "Semantic similarity is the degree to which the meaning of words are alike. This should not be confused with word association, which is the degree to which words are associated with each other, like 'cup' and 'coffee'. These are two very different things, but humans associate them strongly with each other. Some aplications choose to not make this distinction between word association and similarity."
            ],
            "category": "similarity/association distinction"
        },
        {
            "num": 1,
            "questions": [
                "What is the tf-idf algorhithm"
            ],
            "responses": [
                "The tf-idf ('term-frequency' and 'inverse document frequency') alorithm is a way of weighing the information in the vector represention of a word. Words that occur in many ofthe documents considered (like 'The', and 'and') weigh less than words that only occur in a few documents, about a certain topic for instance. Since the basicvector representation only shows if a word occurs in a text, it can be helpful to apply weights to the vector. In short, the weightsare computed by multiplying the frequency of the target word in all documents by the number of documents the word occurs in."
            ],
            "category": "tf-idf algorhithm"
        },
        {
            "num": 1,
            "questions": [
                "What is concordance in NLTK"
            ],
            "responses": [
                "NLTK offers concordance function to search for a series of phrases that contain particular keywords in a text. The function concordance() locates and prints the series of phrases that contain the keyword."
            ],
            "category": "concordance in nltk"
        },
        {
            "num": 1,
            "questions": [
                "What is skip-gram"
            ],
            "responses": [
                "It predicts the context: Skip-gram is one of the unsupervised learning techniques used to find the most related words for a given word. Skip-gram is used to predict the context word for a given target word. It's the reverse of the CBOW algorithm. Here, the target word is input while context words are output. As there is more than one context word to be predicted which makes this problem difficult."
            ],
            "category": "skip-gram"
        },
        {
            "num": 1,
            "questions": [
                "What is continuousBoW"
            ],
            "responses": [
                "It predicts the word: In the continuous bag of words model, context is represented by multiple words for a given target word. For example, we could use “duck” and “lake” as context words for “swam” as the target word. This calls for a modification to the neural network architecture. The modification consists of replicating the input to hidden layer connections x times, the number of context words, and adding a divide by x operation in the hidden layer neurons."
            ],
            "category": "continuousbow"
        },
        {
            "num": 1,
            "questions": [
                "What is a semantic field?"
            ],
            "responses": [
                "A semantic field is the case when multiple lexical elements have a certain form of correlation with eachother, but aren't synonyms of eachother. "
            ],
            "category": "semantic field"
        },
        {
            "num": 1,
            "questions": [
                "What is tf-idf algorithm "
            ],
            "responses": [
                "Tf-idf algorithm consists of two factors, the frequency of a word in a document and the inverse document frequency (IDF)"
            ],
            "category": "tf-idf algorithm"
        },
        {
            "num": 1,
            "questions": [
                "What does distributional mean?"
            ],
            "responses": [
                "It is an approach to characterising the semantic nature of words where the distribution of words plays a role. There is a distributional hypothesis, which looks at the links between the distribution of words and similarity in terms of meaning."
            ],
            "category": "distributional"
        },
        {
            "num": 1,
            "questions": [
                "What is co-occurrence?"
            ],
            "responses": [
                "The previously explained vectors are based on a co-occurrence matrix. This is a way to analyse co-occurrence instances of words. There multiple variations of co-occurrence matrixes, such as, term-document matrix. The hypothesis of co-occurrence analyses whether semantic associations are linked to the co-occurrence of a word."
            ],
            "category": "co-occurrence"
        },
        {
            "num": 1,
            "questions": [
                "IR",
                "What is information retrieval?"
            ],
            "responses": [
                "Information retrieval method is used to research, manipulate and store relevant information in a series of documents through the use of (mostly) sparse vectors."
            ],
            "category": "information retrieval"
        },
        {
            "num": 1,
            "questions": [
                "vector semantics",
                "Distributional method",
                "Distributional methods"
            ],
            "responses": [
                "The meaning of a word is calculated from the surrounding words represented by numbers in vectors. Such vector representation is called an embedding."
            ],
            "category": "distributional method or vector semantics"
        },
        {
            "num": 1,
            "questions": [
                "Term-document matrix"
            ],
            "responses": [
                "A matrix with the words of a vocabulary as the rows and the count of that word in a document per document per column. The vector that’s created by this can be found adding all the counts of words of one document (one column) to a vector. This vector represents the document."
            ],
            "category": "term-document matrix"
        },
        {
            "num": 1,
            "questions": [
                "Word-context matrix"
            ],
            "responses": [
                "Matrix with the target words of a vocabulary as the rows and the context terms in the vocabulary as columns. The number of times a target word is found surround by the context term is counted in respect to a specified distance of the word. The vector, created by adding all the counts of one word (one column) to a vector, represents the word. If two word vectors look alike the target words are similar."
            ],
            "category": "word-context matrix"
        },
        {
            "num": 1,
            "questions": [
                "co-occurence",
                "First order co-occurrence"
            ],
            "responses": [
                "Words that commonly occur nearby each other, for example book and read."
            ],
            "category": "first order co-occurrence"
        },
        {
            "num": 1,
            "questions": [
                "PMI",
                "Pointwise Mutual Information",
                "Pointwise Mutual Information method"
            ],
            "responses": [
                "Instead of just counting the occurrences of a target word surrounded by a context word a weighting is applied. This takes out the possibility of chance and thus lowers the count of uninformative words such as ‘the’. Positive PMI does not allow negative values but replaces them by 0."
            ],
            "category": "pointwise mutual information method"
        },
        {
            "num": 1,
            "questions": [
                "Association",
                "relatedness"
            ],
            "responses": [
                "The extent to which words often occur together in space and language"
            ],
            "category": "assocation"
        },
        {
            "num": 1,
            "questions": [
                "Dependency-based",
                "Dependency-based input"
            ],
            "responses": [
                "Word pairs that have a dependency relation are used as input for a model"
            ],
            "category": "dependency-based input"
        },
        {
            "num": 1,
            "questions": [
                "POS-based",
                "POS-based input",
                "Part-of-speech-based",
                "Part-of-speech-based input"
            ],
            "responses": [
                "Word pairs that are highly associated with the same POS-category: noun, verb and adjectives are used as an input for testing models."
            ],
            "category": "pos-based input"
        },
        {
            "num": 1,
            "questions": [
                "What is semantic representation"
            ],
            "responses": [
                "Semantic representation is the way in which meanings are represented"
            ],
            "category": "semantic-representation"
        },
        {
            "num": 1,
            "questions": [
                "What is representation learning"
            ],
            "responses": [
                "Representation learning is a form of machine learning where features are learnt without supervision"
            ],
            "category": "representation-learning"
        },
        {
            "num": 1,
            "questions": [
                "What is principle of contrast"
            ],
            "responses": [
                "Principle of contrast is the assumption that no two synonym words are absolutely identical in meaning. Some difference in linguistic form must have at least some difference in meaning."
            ],
            "category": "principle-of-contrast"
        },
        {
            "num": 1,
            "questions": [
                "What is a tf-idf model?"
            ],
            "responses": [
                "A tf-idf model defines the meaning of a word by combining term frequency (frequency of a word in a document) with the inverse document frequency, which ensures that words that only occur in a few documents (and thus are more discriminative) get higher weights."
            ],
            "category": "tf-idf model"
        },
        {
            "num": 1,
            "questions": [
                "What is similarity?",
                "What is association?",
                "What is the difference between association and similarity?"
            ],
            "responses": [
                "Two words can be similar if they share physical features, a funcion or if they are clearly in the same category. However, words that often occur in the same linguistic or physical environment but are not similar are associated. "
            ],
            "category": "association_and_similarity"
        },
        {
            "num": 1,
            "questions": [
                "What is weak distributional hypothesis?",
                "What is strong distributional hypothesis?",
                "What is the difference between weak and strong distributional hypothesis?"
            ],
            "responses": [
                "The weak distributional hypothesis entails that semantic content and linguistic distribution have a correlation and studying the distritbution can help us analyse the use of words. However, the strong distributional hypothesis suggests that linguistic ditribution has a causal role in learning the semantic content of a word as it helps us build a semantic representation."
            ],
            "category": "weak_strong_dh"
        },
        {
            "num": 1,
            "questions": [
                "What is the vector space"
            ],
            "responses": [
                "The vector space is a set of vectors, lists/arrays of numbers, in a number of dimensions."
            ],
            "category": "vector space"
        },
        {
            "num": 1,
            "questions": [
                "What is Cosine for measuring similarity"
            ],
            "responses": [
                "to measure two words similarity, should measure the similarity between the two representative vectors"
            ],
            "category": "cosine for measuring similarity"
        },
        {
            "num": 1,
            "questions": [
                "What are word embeddings"
            ],
            "responses": [
                "Word embeddings are representations of meanings of words by vectors."
            ],
            "category": "word embeddings"
        },
        {
            "num": 1,
            "questions": [
                "What is DH",
                "What is Distributional representation",
                "What is the fundament of distributional representations"
            ],
            "responses": [
                "Distributional representations is to analyze word meaning by analyzing the context which the word appears. The fundament of distributional representations is so-called Distributional Hypothesis(DH)."
            ],
            "category": "distributional representation"
        },
        {
            "num": 1,
            "questions": [
                "what is vector representation of words"
            ],
            "responses": [
                "Vector representation of words is to represent meaning of words by using vectors."
            ],
            "category": "vector representation of words"
        },
        {
            "num": 1,
            "questions": [
                "What is the cosine",
                "What is the dot product"
            ],
            "responses": [
                "The dot product is used to compute the similarity between vectors. The cosine is a normalized version of the dot product (i.e., the dot product is divided by the two vector lengths) which is used to compute word and document similarity. This normalization is needed because more frequent words have longer vectors, so dividing by vector length shows the similarity between words regardless of their frequency"
            ],
            "category": "dot-product"
        },
        {
            "num": 1,
            "questions": [
                "What is a word-word matrix",
                "What is a co-occurrence matrix",
                "What is a term-document matrix"
            ],
            "responses": [
                "A co-occurrence matrix shows how often words co-occur. There are different types: a term-document matrix (with D columns and |V| rows) shows how often certain words occur in certain documents, while a word-word matrix (with |V| columns and |V| rows) shows how often certain words co-occur in some training corpus"
            ],
            "category": "co-occurrence-matrix"
        },
        {
            "num": 1,
            "questions": [
                "What is a semantic frame"
            ],
            "responses": [
                "The semantic frame describes the perspective and the actors that are involved in a certain event. For example, when paying for groceries at the counter. There is someone who buys, someone who sells. These two words describe the same event, but from a different perspective."
            ],
            "category": "semanticframe"
        },
        {
            "num": 1,
            "questions": [
                "What is IR",
                "What is information retrieval"
            ],
            "responses": [
                "In information retrieval, there is a certain question that has to be answered and a certain set of documents from which the answer should be retrieved. The task is to find the specific document that helps answering the question best."
            ],
            "category": "ir"
        },
        {
            "num": 1,
            "questions": [
                "What is a distributional model?",
                "What are distributional models?"
            ],
            "responses": [
                "Distributional models quantify the similarity between words. Within the linguistic field, most distributional models are vectors."
            ],
            "category": "distributional models"
        },
        {
            "num": 1,
            "questions": [
                "What is distributional meaning? What is context-theoretic meaning? What is corpusbased meaning? What is statistical meaning?"
            ],
            "responses": [
                "The hypothesis that words that statistically occurring near each other in a text can be used to derive their meaning.."
            ],
            "category": "distributional semantic representation, context-theoretic, corpusbased, statistical semantic approach"
        },
        {
            "num": 1,
            "questions": [
                "What is cross validation?"
            ],
            "responses": [
                "Cross validation is a technique used for evaluating machine learning models to detect for overfitting."
            ],
            "category": "cross validation"
        },
        {
            "num": 1,
            "questions": [
                "What is a tf-idf model used for?"
            ],
            "responses": [
                "This model is used to compute word similarity using the frequency of a word existing with a neighbour word. "
            ],
            "category": "vector models"
        },
        {
            "num": 1,
            "questions": [
                "What is association"
            ],
            "responses": [
                "Association is a relationship between words where words often appear in similar contexts, despite their referents being functionally dissimilar, such as ‘coffee’ and ‘cup’, or ‘car’ and ‘petrol’."
            ],
            "category": "association"
        },
        {
            "num": 1,
            "questions": [
                "What is cosine similarity"
            ],
            "responses": [
                "Cosine similarity is a measure of similarity between two vectors that takes into account the length of the vectors and the angle between them. This measure gives insight into how close two words are to one another inside a vector space, and, consequently, how similar their meanings are according to distributional semantics."
            ],
            "category": "cosine-similarity"
        },
        {
            "num": 1,
            "questions": [
                "cosine",
                "vector similarity"
            ],
            "responses": [
                "The similarity of two word vectors is most commonly calculated by the cosine of the angles of the two vectors. This can be done even with multidimensional vectors by taking the dot product of the two vectors normalizing by dividing the resulting number by the lengths of the two vectors. The maximum value of this cosine is 1 and the minimum (as it is calculated from frequency counts) is 0 and the closer the cosine is to 1, the more similar the two word vectors are."
            ],
            "category": "cosine"
        },
        {
            "num": 1,
            "questions": [
                "similarity",
                "association",
                "word relations"
            ],
            "responses": [
                "Word vector similarity can easily be calculated by taking the cosine of the vectors. However, it is not clear what this word vector similarity really captures. Often words such as ‘thermos’ and ‘tea’ have high a vector similarity even though ‘thermos’ and ‘tea’ are not similar concepts at all. They are in close association, however, as they belong to the same semantic field. Distinguishing between similarity and association is a crucial but challenging task in applications that use vector semantics."
            ],
            "category": "similarity and association"
        },
        {
            "num": 1,
            "questions": [
                "What is  a dual model"
            ],
            "responses": [
                "There are arguments to give that distributional semantics is not semantics (compositionality, lexical inference, reference and grounding). There is also a trend in cognitive sciences to find a common ground in which embodied cognition and distributional approaches to meaning could eventually meet.  "
            ],
            "category": "dual model"
        },
        {
            "num": 1,
            "questions": [
                "What is skip gram?"
            ],
            "responses": [
                "Skip gram is a algorithm in the software package word2vec that is used in dense data sets to determine short vectors. It is based on a binary prediction of likelihood for a word x to occur near word y. This is different from other vector prediction models that are based on co-occurrence."
            ],
            "category": "skip gram"
        },
        {
            "num": 1,
            "questions": [
                "What is a vector"
            ],
            "responses": [
                "A vector is the calculated value of a word"
            ],
            "category": "vector "
        },
        {
            "num": 1,
            "questions": [
                "What is Word Relatedness"
            ],
            "responses": [
                "Word Relatedness is when two words relate to one another conceptually, for example, coffee and cup are related with one another, even though the words are not similar to each other"
            ],
            "category": "word relatedness"
        },
        {
            "num": 1,
            "questions": [
                "What is a Semantic Field"
            ],
            "responses": [
                "Semantic Field is a type of relatedness that shares a domain, for example, school (teacher, student, books, subjects)"
            ],
            "category": " semantic field"
        },
        {
            "num": 1,
            "questions": [
                "What is the Distributional Hypothesis "
            ],
            "responses": [
                "A hypothesis that states that there is a link between the context of words and the meaning of words. Two words that occur in similar contexts have, according to this hypothesis, a similar meaning. "
            ],
            "category": "weak-distributional-hypothesis"
        },
        {
            "num": 1,
            "questions": [
                "What is the strong distributional hypothesis"
            ],
            "responses": [
                "This is a similar hypothesis to the weak DH, also called weak DH. However, in the strong version it is claimed that this is grounded in cognition. There is a causal relation between the meaning of a word, and its embedded words. It can be said that the weak distributional hypothesis states that meaning can be inferred from how words are distributed, and the strong version states that the distribution can also be inferred from the meaning of the words."
            ],
            "category": "strong-distributional-hypothesis"
        },
        {
            "num": 1,
            "questions": [
                "What are Vector Semantic Models"
            ],
            "responses": [
                "A model that learns the meaning of words by reading its context and representing the words as vectors. These models can be divided into two: sparse and dense. A sparse model uses very long vectors, which are predominantly filled with zeroes. A dense model on the other hand uses vectors where most values are non-zero."
            ],
            "category": "vector-semantic-models"
        },
        {
            "num": 1,
            "questions": [
                "What is likelihood ratio?",
                "Wat is a likelihood ratio?"
            ],
            "responses": [
                "The likelihood ratio describes the probability that stimuli belong to one of the categories defined. A likelihood ratio of 1:3 means that the probability of something happening is 3 times bigger than the other thing happening."
            ],
            "category": "likelihood ratio"
        },
        {
            "num": 1,
            "questions": [
                "What are hidden Markov models?",
                "What is a hidden Markov model?"
            ],
            "responses": [
                "In hidden Markov models, there are several parameters in a formula, some of them hidden. With this statistical method, the goal is to calculate the unknown parameters witht the help of the known ones."
            ],
            "category": "hidden markov models"
        },
        {
            "num": 1,
            "questions": [
                "What is Bayesian inference?",
                "What is bayesian inference?"
            ],
            "responses": [
                "Bayesian inference is a statistical method in which Bayes' therom is used to continuously update the probability of the determined null-hypothesis when more information becomes available."
            ],
            "category": "bayesian inference"
        },
        {
            "num": 1,
            "questions": [
                "What is Confusion Matrices"
            ],
            "responses": [
                "is a matrix one of its diagonal represent the error in prediction and the other represent the correct prediction"
            ],
            "category": "confusion matrices"
        },
        {
            "num": 1,
            "questions": [
                "What is Precision"
            ],
            "responses": [
                "the number of true positives/the number of true positives plus the number of false positives"
            ],
            "category": "precision "
        },
        {
            "num": 1,
            "questions": [
                "What is F1-measure"
            ],
            "responses": [
                "the componitation of the recall and precision"
            ],
            "category": "f1-measure"
        },
        {
            "num": 1,
            "questions": [
                "What is Statistical Significance Testing"
            ],
            "responses": [
                "to prove that the final result have been founded on top of facts not just by luck"
            ],
            "category": "statistical significance testing"
        },
        {
            "num": 1,
            "questions": [
                "What is Error Analysis"
            ],
            "responses": [
                "Error Analysis is a method that refines the feature set by using a development set, which contains the corpora, which is then divided into a training set and a dev-test. The latter performs the error analysis"
            ],
            "category": "error analysis"
        },
        {
            "num": 1,
            "questions": [
                "What is Feature Extractor"
            ],
            "responses": [
                "A Feature Extractor tells the classifier which aspects or features should be selected"
            ],
            "category": "feature extractor"
        },
        {
            "num": 1,
            "questions": [
                "What is a Naive Bayes Classifier"
            ],
            "responses": [
                "A supervised learning classifier that considers every feature in the deciding the label of the input. It makes use of a prior probability of each label, which is based on its frequency in the traning data set. These probabilities are then updated based on each of the features."
            ],
            "category": "naive bayes classifier "
        },
        {
            "num": 1,
            "questions": [
                "What is the Presision metric"
            ],
            "responses": [
                "A measure of performance of a bayesian classifier. It shows the amount of true positives (correctly identified trues) as a percentage of all that is idententified as true (true + false positives). Combined with the recall measure, this makes the F-measure."
            ],
            "category": "presision"
        },
        {
            "num": 1,
            "questions": [
                "What is feature selection"
            ],
            "responses": [
                "Feature selection is the process of eliminating feature that aren't usefull for the generalisation of the model."
            ],
            "category": "feature selection"
        },
        {
            "num": 1,
            "questions": [
                "What is bootstrapping"
            ],
            "responses": [
                "Bootstrapping is a way to recieve a version of sampling that works by selecting items for small samples using replacement."
            ],
            "category": "bootstrapping"
        },
        {
            "num": 1,
            "questions": [
                "multinomial classification",
                "classifying into multiple classes"
            ],
            "responses": [
                "When a text classification task requires more than two class labels to be considered for each target item, the most common solution in NLP is multinomial classification. Binary classifiers are trained for each class label (where positives are all instances of items being members of the target class and negatives are instances of items not being members of the class), and then whichever class label received the highest score from its classifier, that is the class the target item will be assigned to."
            ],
            "category": "multinomial classification"
        },
        {
            "num": 1,
            "questions": [
                "What are descriptive models?"
            ],
            "responses": [
                "Descriptive models try to capture patterns in given data, but they don't provide any information about the reasons why the given data contains those patterns. "
            ],
            "category": "descriptive models"
        },
        {
            "num": 1,
            "questions": [
                "What are explanatory models?"
            ],
            "responses": [
                "Explanatory models are distinctive from descriptive models, they try to capture properties and relations that are the reason/cause the linguistic patterns. "
            ],
            "category": "explanatory models"
        },
        {
            "num": 1,
            "questions": [
                "What are generative classifiers?"
            ],
            "responses": [
                "Generative classifiers (for instance Naive Bayes) construct a model of how a class could generate input data. Given a observation, they return the class which is most likely to have generated the classifiers. "
            ],
            "category": "generative classifiers"
        },
        {
            "num": 1,
            "questions": [
                "What is a contigency table?"
            ],
            "responses": [
                "A contigency table is a table that reflects the outcomes of two measuring systems. It shows four types of outcomes: true positives, true negatives, false positives (type I) and false negatives (type II)."
            ],
            "category": "contigency table"
        },
        {
            "num": 1,
            "questions": [
                "What is text classification"
            ],
            "responses": [
                "Text classification is the task of appointing a label or category to a text"
            ],
            "category": "text classification"
        },
        {
            "num": 1,
            "questions": [
                "What is sentiment lexicon"
            ],
            "responses": [
                "Sentiment lexicon is a list of words with a predefined positive or negative emotion (sentiment). It is mainly used in sentiment analysis"
            ],
            "category": "sentiment lexicon"
        },
        {
            "num": 1,
            "questions": [
                "spam",
                "email",
                "what is spam detection?"
            ],
            "responses": [
                "Spam detection is a binary classification task that assigns a spam or not-spam label to an email. It is widely adopted for commercial applications."
            ],
            "category": "spam detection"
        },
        {
            "num": 1,
            "questions": [
                "What is Linguistic inquiry and word count?"
            ],
            "responses": [
                " LIWC is a text analysis program that takes a given piece of text and calculated the percentage of words that fall in a certain category. For this program, 90 linguistic, psychological and topical categories are used to indicate social, cognitive and affective processes. The program uses a dictionary that contains words and their score in different languages as a core. By making use of Linguistic Inquiry and Word Count (LIWC) the degree in which a text is positive or negative can be calculated."
            ],
            "category": "linguistic “inquiry and word count"
        },
        {
            "num": 1,
            "questions": [
                "What are One-hot-encodings?"
            ],
            "responses": [
                "One-hot-encoding is a way of encoding different elements in a vector. The method of one-hot-encodings creates additional features based on the number of unique values in the categorical feature. Every unique value in the category will be added as a feature. Because many machine learning agents cannot deal with textual data directly, the data has to be presented in a numerical way. One-hot-encodings are a way of converting elements or categories into numbers. There are some drawbacks of one-hot-encodings such as difficulties to generalize over the features, and that one-hot encodings are not very efficient in use. Finally, a lot of trainings data is needed to find sufficient examples."
            ],
            "category": " 'one-hot-encodings' "
        },
        {
            "num": 1,
            "questions": [
                "What are the different types of data that can be fed to a machine?"
            ],
            "responses": [
                "When talking about machine learning, we can distinguish different types of data based on their quality, reliability and integrity. The data can be categorized by the three different types of Gold, Silver and Bronze data. When data is generated by a machine and not validated; we call it bronze data. This type of data is ‘bring your own’ user data because users load their own raw data, perform cleaning and stripping tasks to it and then use this for their analytic tasks. Silver data is normally cleansed by a machine and checked by a human. The golden type of data is actually silver data that has been further refined by human annotators, an example is IOB annotations for entities."
            ],
            "category": "gold, silver and bronze data"
        },
        {
            "num": 1,
            "questions": [
                "What are the different NLP approaches?"
            ],
            "responses": [
                "There are three different Natural Language Processing approaches: The rule-based approach in which machines are told exactly what to do under specific circumstances. This approach is the oldest approach in natural language processing and tends to focus on the pattern-matching or parsing. Rule-based approaches are low in precision when generalized but have a high performance in specific cases. The second approach to NLP is Machine learning. This traditional approach is characterized by the use of training data, such as a corpus, and is based on the idea that machines learn to associate patterns with interpretation. There are two subcategories for the machine learning approach: supervised and unsupervised machine learning. As the name already suggests uses the machine in supervised machine learning labelled examples to learn, and with unsupervised machine learning, the machine identifies patterns without using labelled examples. The last approach to NLP is Hybrid in which a combination is made between (un)supervised machine learning and rule-based approaches."
            ],
            "category": "nlp approaches"
        },
        {
            "num": 1,
            "questions": [
                "What is the difference between micro and macro averaging?"
            ],
            "responses": [
                " When computing averages, there are different ways in which the average of a certain class can be computed. In machine learning there are two types of averaging: micro and macro averaging. A macro-average will compute the metric independency, which means that all classes are equally treated. Where on the other hand micro-averaging will take the contribution of all classes into account when computing an average metric. When computing the average of different labels that are unequally divided in terms of occurrences, and the user wants to bias its metric toward the most populated: use micro. When the user wants to bias the metric towards the least populated occurrence: use macro.  In other words: One talks about macro averaging when the performance is balanced across classes and we talk about micro averaging if the number of cases is balanced across classes. "
            ],
            "category": "micro and macro averaging"
        },
        {
            "num": 1,
            "questions": [
                "What is sentiment classification"
            ],
            "responses": [
                "Sentiment classification is one of the most common text categorization tasks. Sentiment analysis is made to find out the positive or negative orientation that a writer expresses toward some object. The goal of classification is to take a single observation, extract some useful features, and thereby classify the observation into one of a set of discrete classes."
            ],
            "category": "sentiment classification"
        },
        {
            "num": 1,
            "questions": [
                "What is stop words"
            ],
            "responses": [
                "In NLP it is required the data to be preprocessed to be able to apply further analysis. Raw data includes words like 'the', 'a/an' which do not make any difference for sentiments analysis. Stop words removes the items that are defined as unnecessary, from the training data."
            ],
            "category": "stop words"
        },
        {
            "num": 1,
            "questions": [
                "What is the general inquirer LIWC"
            ],
            "responses": [
                "Four popular lexicons are the General Inquirer (Stone et al., 1966), LIWC (Pennebaker et al., 2007), the opinion lexicon General Inquirer LIWC of Hu and Liu (2004a) and the MPQA Subjectivity Lexicon (Wilson et al., 2005). "
            ],
            "category": "general inquirer liwc"
        },
        {
            "num": 1,
            "questions": [
                "What is a test set"
            ],
            "responses": [
                "When training a model to use as a classifier, a test set is required to measure the accuracy of the classifier. This is a seperate set that is not used for training."
            ],
            "category": "test-set"
        },
        {
            "num": 1,
            "questions": [
                "What is supervised classification",
                "What is supervised machine learning"
            ],
            "responses": [
                "In supervised machine learning, a machine is trained by a training corpus containing certain inputs (e.g., texts) with labels, from which the machine learns how to correctly classify new unseen input"
            ],
            "category": "supervised-machine-learning"
        },
        {
            "num": 1,
            "questions": [
                "What is feature selection",
                "What is feature engineering"
            ],
            "responses": [
                "Feature engineering is the use of features (properties of the input) to represent input variables (e.g., texts) and to predict output variables (e.g., classes/labels). Feature selection is used to remove irrelevant or uninformative features from a representation"
            ],
            "category": "feature-engineering"
        },
        {
            "num": 1,
            "questions": [
                "What is classification",
                "What is supervised classification"
            ],
            "responses": [
                "Supervised classification is assigning a label to a text or document based on trained input-output data."
            ],
            "category": "supervised-classification"
        },
        {
            "num": 1,
            "questions": [
                "What is authorship attribution"
            ],
            "responses": [
                "Authorship attribution is identifying the author of a text or an author's characteristics."
            ],
            "category": "authorship-attribution"
        },
        {
            "num": 1,
            "questions": [
                "What is naive bayes",
                "What is naive bayes classifier"
            ],
            "responses": [
                "Naive Bayes classficer is a simple probabilistic classifier that labels a text based on the probability of each label and the features' contribution."
            ],
            "category": "naive-bayes-classifier"
        },
        {
            "num": 1,
            "questions": [
                "What is a decision tree"
            ],
            "responses": [
                "A decision tree is a flowchart of decisions and their outcomes depending on different conditions."
            ],
            "category": "decision-trees"
        },
        {
            "num": 1,
            "questions": [
                "what is the naive bayes assumption"
            ],
            "responses": [
                "The naive bayes assumption is the assumption that words are conditionally independent of each other given the class."
            ],
            "category": "naive bayes assumption"
        },
        {
            "num": 1,
            "questions": [
                "What is a supervised classifier"
            ],
            "responses": [
                "A classifier is called supervised if it is trained using data set where the correct output is given for each piece of input."
            ],
            "category": "supervised-classifier"
        },
        {
            "num": 1,
            "questions": [
                "What is a classifier "
            ],
            "responses": [
                "In machine learning classification is any algorithm that predicts the labeled classes of the data (in NLP the text). A version is supervised classification. During the traine phase a feautyre extractor is usd to buil a feauture set. These sets are put in the machie learning algorithm together with the label. In the prediction phase the same feauture extrator is used to convert input into feauture sets. The feauture set are put into the classifier model which predicts the labels."
            ],
            "category": "classifier "
        },
        {
            "num": 1,
            "questions": [
                "What is the difference between these sets"
            ],
            "responses": [
                "A method to refine a feauture set is error analysis. Normally there is a tarining data set and a test data set to meausure the peformance of the classifier model. When there are more than two classifications a third data set is needed. The dev-set is added to the developmetn set (together with the training set) to perform error analysis on the individual feautures."
            ],
            "category": "difference training, dev and test sets"
        },
        {
            "num": 1,
            "questions": [
                "What is unsupervised machine learning"
            ],
            "responses": [
                "An algorithm that learns based on a unlabeled dataset, meaning that the set contains instances without right answers and will be clustered."
            ],
            "category": "unsupervised machine learning"
        },
        {
            "num": 1,
            "questions": [
                "What is one-hot encoding"
            ],
            "responses": [
                "A way of feature representation to prevent the algorithm to learn that higher is better, providing an equal playing field."
            ],
            "category": "one-hot encoding"
        },
        {
            "num": 1,
            "questions": [
                "F-1",
                "recall",
                "precision"
            ],
            "responses": [
                "a mixture of precision and recall",
                "the ratio between the selected and the relevant selected items",
                "the ratio of items relevant items selected and the total selected"
            ],
            "category": "measures"
        },
        {
            "num": 1,
            "questions": [
                "What is rule-based approach"
            ],
            "responses": [
                "An algorithm that acts based on the hardcoded rules that apply to each situation."
            ],
            "category": "rule-based approach"
        },
        {
            "num": 1,
            "questions": [
                "What is sequence classification?"
            ],
            "responses": [
                "Sequence classification is performed by different models, the main idea of which is to label, for example in the case of POS tagging, a sequence of input, such as a sentence. In some models, the classification of an input is influenced by how the previous input was classified. This could lead to a chain of mis-labeling and some models are more fine-tuned, like the Hidden Markov model where all of the input is classified multiple times and then probabilities scores are claculated and the best option is chosen."
            ],
            "category": "sequence-classification"
        },
        {
            "num": 1,
            "questions": [
                "What can be a linguistics feature?",
                "What are linguistic features used for?"
            ],
            "responses": [
                "Linguistics features are elements of language that are used to classify and analyze text. It could be certain expressions, for example in the case of spam detection, or parts of speech, word length, etc. "
            ],
            "category": "linguistic-feature"
        },
        {
            "num": 1,
            "questions": [
                "What are sentiment lexicons?"
            ],
            "responses": [
                "These are lists of words that already have either a positive or negative sentiment. There are four popular lexicons: General inquirer, LIWC, the opinion lexicon of Hu and Lui and the MPQA Subjectivity lexicon. Lexicons like these are commonly used to add a feature that counts the occurrence of a word in the lexicon."
            ],
            "category": "sentiment lexicons"
        },
        {
            "num": 1,
            "questions": [
                "What are multi-class classification tasks?"
            ],
            "responses": [
                "There are two types of classification tasks: multi-label classification (any-of) and multinomial classification (one-of). In the multi-label classification task, multiple labels can be assigned to a document or an item, while in multinomial classification a document or an item only occurs in one class, so there is a mutual exclusivity for classes. The latter is more used in language processing."
            ],
            "category": "multi-class classification tasks"
        }
    ]
}